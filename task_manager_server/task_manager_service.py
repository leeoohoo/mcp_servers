#!/usr/bin/env python3
"""
ä»»åŠ¡ç®¡ç†æœåŠ¡æ¨¡å—
æä¾›ä»»åŠ¡åˆ›å»ºã€æŸ¥è¯¢ã€çŠ¶æ€ç®¡ç†ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
import uuid
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, AsyncGenerator
from dataclasses import dataclass, asdict

logger = logging.getLogger("task_manager_service")


@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®æ¨¡å‹"""
    id: str
    task_title: str
    target_file: str
    operation: str
    specific_operations: str
    related: str
    dependencies: str
    conversation_id: str
    request_id: str
    status: str = "pending"  # pending, in_progress, completed
    created_at: str = None
    updated_at: str = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now().isoformat()
        if self.updated_at is None:
            self.updated_at = datetime.now().isoformat()


class TaskManagerService:
    """ä»»åŠ¡ç®¡ç†æœåŠ¡ç±»
    
    è´Ÿè´£ä»»åŠ¡çš„åˆ›å»ºã€åŠ è½½ã€ä¿å­˜ã€æŸ¥è¯¢å’ŒçŠ¶æ€ç®¡ç†ã€‚
    é‡‡ç”¨å®Œå…¨æŒ‰éœ€åŠ è½½çš„æ–¹å¼ï¼Œæ¯æ¬¡æ“ä½œæ—¶ç›´æ¥ä»æ–‡ä»¶è¯»å–ï¼Œä¸ç»´æŠ¤å†…å­˜ç¼“å­˜ã€‚
    """
    
    def __init__(self, data_dir: str = "./task_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.auto_save = True
        
        logger.info(f"TaskManagerService initialized with data dir: {self.data_dir.absolute()}")
    
    def _get_data_file_path(self, conversation_id: str, request_id: str) -> Path:
        """è·å–æ•°æ®æ–‡ä»¶è·¯å¾„"""
        filename = f"{conversation_id}_{request_id}.json"
        return self.data_dir / filename
    

    
    def _load_tasks_from_file(self, file_path: Path) -> Dict[str, Task]:
        """ä»æŒ‡å®šæ–‡ä»¶åŠ è½½ä»»åŠ¡æ•°æ®"""
        tasks = {}
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for task_data in data.get('tasks', []):
                        task = Task(**task_data)
                        tasks[task.id] = task

            except Exception as e:
                logger.error(f"åŠ è½½ä»»åŠ¡æ•°æ®å¤±è´¥ {file_path}: {e}")
        return tasks
    
    def _save_tasks_to_file(self, conversation_id: str, request_id: str, tasks: List[Task]):
        """ä¿å­˜ä»»åŠ¡æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶"""
        try:
            file_path = self._get_data_file_path(conversation_id, request_id)
            data = {
                'conversation_id': conversation_id,
                'request_id': request_id,
                'tasks': [asdict(task) for task in tasks],
                'updated_at': datetime.now().isoformat()
            }
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {len(tasks)} ä¸ªä»»åŠ¡åˆ° {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡æ•°æ®å¤±è´¥: {e}")
    
    def _get_task_by_id(self, task_id: str) -> Optional[Task]:
        """æ ¹æ®IDè·å–ä»»åŠ¡"""
        # éå†æ‰€æœ‰æ–‡ä»¶æŸ¥æ‰¾
        for data_file in self.data_dir.glob("*.json"):
            file_tasks = self._load_tasks_from_file(data_file)
            if task_id in file_tasks:
                return file_tasks[task_id]
        
        return None
    
    def _get_tasks_by_conversation_request(self, conversation_id: str, request_id: str) -> List[Task]:
        """è·å–æŒ‡å®šä¼šè¯å’Œè¯·æ±‚çš„ä»»åŠ¡"""
        file_path = self._get_data_file_path(conversation_id, request_id)
        file_tasks = self._load_tasks_from_file(file_path)
        return list(file_tasks.values())
    

    
    async def create_tasks_stream(self, tasks_data: List[Dict[str, Any]], 
                                conversation_id: str, request_id: str) -> AsyncGenerator[str, None]:
        """åˆ›å»ºä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºï¼‰"""
        yield f"å¼€å§‹åˆ›å»º {len(tasks_data)} ä¸ªä»»åŠ¡...\n"
        
        created_tasks = []
        errors = []
        
        for i, task_data in enumerate(tasks_data):
            try:
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = [
                    'task_title', 'target_file', 'operation', 
                    'specific_operations', 'related', 'dependencies'
                ]
                
                missing_fields = [field for field in required_fields if field not in task_data]
                if missing_fields:
                    errors.append({
                        'index': i,
                        'error': f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}"
                    })
                    continue
                
                # åˆ›å»ºä»»åŠ¡
                task_id = str(uuid.uuid4())
                task = Task(
                    id=task_id,
                    task_title=task_data['task_title'],
                    target_file=task_data['target_file'],
                    operation=task_data['operation'],
                    specific_operations=task_data['specific_operations'],
                    related=task_data['related'],
                    dependencies=task_data['dependencies'],
                    conversation_id=conversation_id,
                    request_id=request_id,
                    status=task_data.get('status', 'pending')
                )
                
                created_tasks.append(task)
                
                yield f"[{i+1}/{len(tasks_data)}] åˆ›å»ºä»»åŠ¡: {task.task_title} (ID: {task_id})\n"
                
            except Exception as e:
                error_msg = f"[{i+1}/{len(tasks_data)}] åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}\n"
                yield error_msg
                errors.append({
                    'index': i,
                    'error': str(e)
                })
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if created_tasks:
            self._save_tasks_to_file(conversation_id, request_id, created_tasks)
            yield f"\nâœ… æˆåŠŸåˆ›å»º {len(created_tasks)} ä¸ªä»»åŠ¡å¹¶ä¿å­˜åˆ°æ–‡ä»¶: {conversation_id}_{request_id}.json\n"
        
        if errors:
            yield f"âŒ å¤±è´¥ {len(errors)} ä¸ªä»»åŠ¡\n"
        
        yield f"\nğŸ“Š æ€»ç»“: æˆåŠŸåˆ›å»º {len(created_tasks)} ä¸ªä»»åŠ¡ï¼Œå¤±è´¥ {len(errors)} ä¸ª\n"
    
    async def get_next_executable_task_stream(self, conversation_id: str, request_id: str) -> AsyncGenerator[str, None]:
        """è·å–ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œä»»åŠ¡çš„æµå¼ç‰ˆæœ¬ - å®Œå…¨æŒ‰éœ€åŠ è½½"""
        yield "ğŸ” æ­£åœ¨æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œä»»åŠ¡...\n"
        
        # ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡
        file_path = self._get_data_file_path(conversation_id, request_id)
        if not file_path.exists():
            yield "âŒ æœªæ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶\n"
            return
        
        tasks_dict = self._load_tasks_from_file(file_path)
        if not tasks_dict:
            yield "âŒ ä»»åŠ¡æ–‡ä»¶ä¸ºç©º\n"
            return
        
        # æŸ¥æ‰¾å¯æ‰§è¡Œä»»åŠ¡
        executable_tasks = []
        for task in tasks_dict.values():
            if task.status == 'pending':
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®Œæˆ
                dependencies_met = True
                if task.dependencies and task.dependencies != "æ— ":
                    for dep_id in task.dependencies.split(','):
                        dep_id = dep_id.strip()
                        if dep_id and dep_id in tasks_dict and tasks_dict[dep_id].status != 'completed':
                            dependencies_met = False
                            break
                
                if dependencies_met:
                    executable_tasks.append(task)
        
        if not executable_tasks:
            yield "âŒ æ²¡æœ‰æ‰¾åˆ°å¯æ‰§è¡Œçš„ä»»åŠ¡\n"
            return
        
        # è¿”å›ç¬¬ä¸€ä¸ªå¯æ‰§è¡Œä»»åŠ¡ï¼ˆæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼‰
        next_task = min(executable_tasks, key=lambda t: t.created_at)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        next_task.status = 'in_progress'
        next_task.updated_at = datetime.now().isoformat()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        tasks_to_save = list(tasks_dict.values())
        self._save_tasks_to_file(conversation_id, request_id, tasks_to_save)
        
        yield f"âœ… æ‰¾åˆ°å¯æ‰§è¡Œä»»åŠ¡: {next_task.task_title} (ID: {next_task.id})\n"
        yield f"ğŸ“„ ç›®æ ‡æ–‡ä»¶: {next_task.target_file}\n"
        yield f"ğŸ”§ æ“ä½œç±»å‹: {next_task.operation}\n"
        yield f"ğŸ“ å…·ä½“æ“ä½œ: {next_task.specific_operations}\n"
        yield f"ğŸ”— ç›¸å…³ä¿¡æ¯: {next_task.related}\n"
        yield f"ğŸ“Š ä¾èµ–å…³ç³»: {next_task.dependencies}\n"
        yield f"â–¶ï¸ ä»»åŠ¡å·²æ ‡è®°ä¸ºè¿›è¡Œä¸­\n"
    
    async def complete_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        """å®Œæˆä»»åŠ¡çš„æµå¼ç‰ˆæœ¬ - å®Œå…¨æŒ‰éœ€åŠ è½½"""
        yield f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ä»»åŠ¡ {task_id}...\n"
        
        task_found = None
        conversation_id = None
        request_id = None
        file_path = None
        
        # éå†æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶æŸ¥æ‰¾ä»»åŠ¡
        for file in self.data_dir.glob("*.json"):
            if "_" in file.stem:
                try:
                    tasks_dict = self._load_tasks_from_file(file)
                    if task_id in tasks_dict:
                        task_found = tasks_dict[task_id]
                        parts = file.stem.split('_', 1)
                        conversation_id = parts[0]
                        request_id = parts[1]
                        file_path = file
                        break
                except Exception:
                    continue
        
        if not task_found:
            yield f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨\n"
            return
        
        yield f"ğŸ“‹ æ‰¾åˆ°ä»»åŠ¡: {task_found.task_title}\n"
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_found.status = 'completed'
        task_found.updated_at = datetime.now().isoformat()
        
        # é‡æ–°åŠ è½½æ‰€æœ‰ä»»åŠ¡å¹¶ä¿å­˜
        tasks_dict = self._load_tasks_from_file(file_path)
        tasks_dict[task_id] = task_found
        tasks_to_save = list(tasks_dict.values())
        
        self._save_tasks_to_file(conversation_id, request_id, tasks_to_save)
        yield f"âœ… ä»»åŠ¡ '{task_found.task_title}' å·²æ ‡è®°ä¸ºå®Œæˆ\n"
        yield f"ğŸ’¾ å·²ä¿å­˜åˆ°æ–‡ä»¶: {conversation_id}_{request_id}.json\n"
    
    async def get_task_stats_stream(self, conversation_id: Optional[str] = None) -> AsyncGenerator[str, None]:
        """è·å–ä»»åŠ¡ç»Ÿè®¡çš„æµå¼ç‰ˆæœ¬ - å®Œå…¨æŒ‰éœ€åŠ è½½"""
        yield "ğŸ“Š æ­£åœ¨ç»Ÿè®¡ä»»åŠ¡ä¿¡æ¯...\n"
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡
        all_tasks = []
        for file in self.data_dir.glob("*.json"):
            if "_" in file.stem:
                try:
                    tasks_dict = self._load_tasks_from_file(file)
                    for task in tasks_dict.values():
                        if conversation_id is None or task.conversation_id == conversation_id:
                            all_tasks.append(task)
                except Exception:
                    continue
        
        scope = f"ä¼šè¯ {conversation_id}" if conversation_id else "å…¨éƒ¨"
        yield f"ğŸ” ç»Ÿè®¡èŒƒå›´: {scope}\n"
        
        if not all_tasks:
            yield f"â„¹ï¸ {scope}æ²¡æœ‰ä»»åŠ¡\n"
            return
        
        stats = {
            'total': len(all_tasks),
            'pending': len([t for t in all_tasks if t.status == 'pending']),
            'in_progress': len([t for t in all_tasks if t.status == 'in_progress']),
            'completed': len([t for t in all_tasks if t.status == 'completed'])
        }
        
        yield f"ğŸ“ˆ {scope}ä»»åŠ¡ç»Ÿè®¡:\n"
        yield f"  ğŸ“‹ æ€»è®¡: {stats['total']} ä¸ª\n"
        yield f"  â³ å¾…æ‰§è¡Œ: {stats['pending']} ä¸ª\n"
        yield f"  ğŸ”„ è¿›è¡Œä¸­: {stats['in_progress']} ä¸ª\n"
        yield f"  âœ… å·²å®Œæˆ: {stats['completed']} ä¸ª\n"
        
        if all_tasks:
            yield "\nğŸ“ ä»»åŠ¡åˆ—è¡¨:\n"
            for i, task in enumerate(all_tasks, 1):
                status_emoji = {'pending': 'â³', 'in_progress': 'ğŸ”„', 'completed': 'âœ…'}.get(task.status, 'â“')
                yield f"  {i}. {status_emoji} {task.task_title} (ID: {task.id})\n"
    
    async def query_tasks_stream(self, conversation_id: Optional[str] = None, status: Optional[str] = None, task_title: Optional[str] = None) -> AsyncGenerator[str, None]:
        """æŸ¥è¯¢ä»»åŠ¡çš„æµå¼ç‰ˆæœ¬ - å®Œå…¨æŒ‰éœ€åŠ è½½"""
        yield "ğŸ” æ­£åœ¨æŸ¥è¯¢ä»»åŠ¡...\n"
        
        # æ˜¾ç¤ºæŸ¥è¯¢æ¡ä»¶
        filters = []
        if conversation_id:
            filters.append(f"ä¼šè¯ID: {conversation_id}")
        if status:
            filters.append(f"çŠ¶æ€: {status}")
        if task_title:
            filters.append(f"æ ‡é¢˜å…³é”®è¯: {task_title}")
        
        if filters:
            yield f"ğŸ“‹ æŸ¥è¯¢æ¡ä»¶: {', '.join(filters)}\n"
        else:
            yield "ğŸ“‹ æŸ¥è¯¢æ¡ä»¶: æ— ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡ï¼‰\n"
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡
        all_tasks = []
        for file in self.data_dir.glob("*.json"):
            if "_" in file.stem:
                try:
                    tasks_dict = self._load_tasks_from_file(file)
                    all_tasks.extend(tasks_dict.values())
                except Exception:
                    continue
        
        filtered_tasks = all_tasks
        
        # æŒ‰æ¡ä»¶è¿‡æ»¤
        if conversation_id:
            filtered_tasks = [t for t in filtered_tasks if t.conversation_id == conversation_id]
            yield f"  ğŸ”¸ æŒ‰ä¼šè¯IDè¿‡æ»¤å: {len(filtered_tasks)} ä¸ªä»»åŠ¡\n"
        
        if status:
            filtered_tasks = [t for t in filtered_tasks if t.status == status]
            yield f"  ğŸ”¸ æŒ‰çŠ¶æ€è¿‡æ»¤å: {len(filtered_tasks)} ä¸ªä»»åŠ¡\n"
        
        if task_title:
            filtered_tasks = [t for t in filtered_tasks if task_title.lower() in t.task_title.lower()]
            yield f"  ğŸ”¸ æŒ‰æ ‡é¢˜å…³é”®è¯è¿‡æ»¤å: {len(filtered_tasks)} ä¸ªä»»åŠ¡\n"
        
        yield f"\nğŸ“Š æ‰¾åˆ° {len(filtered_tasks)} ä¸ªåŒ¹é…çš„ä»»åŠ¡\n"
        
        if filtered_tasks:
            yield "\nğŸ“ åŒ¹é…çš„ä»»åŠ¡åˆ—è¡¨:\n"
            for i, task in enumerate(filtered_tasks, 1):
                status_emoji = {'pending': 'â³', 'in_progress': 'ğŸ”„', 'completed': 'âœ…'}.get(task.status, 'â“')
                yield f"  {i}. {status_emoji} {task.task_title}\n"
                yield f"     ğŸ“ æ–‡ä»¶: {task.target_file}\n"
                yield f"     ğŸ”§ æ“ä½œ: {task.operation}\n"
                yield f"     ğŸ†” ID: {task.id}\n"
                yield f"     ğŸ“… åˆ›å»ºæ—¶é—´: {task.created_at}\n\n"
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        else:
            yield "â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä»»åŠ¡\n"
    

    
    def update_data_dir(self, new_data_dir: str) -> Dict[str, Any]:
        """æ›´æ–°æ•°æ®ç›®å½•"""
        try:
            old_dir = self.data_dir
            self.data_dir = Path(new_data_dir)
            self.data_dir.mkdir(exist_ok=True)
            

            
            logger.info(f"æ•°æ®ç›®å½•å·²æ›´æ–°: {old_dir} -> {self.data_dir}")
            return {
                'success': True,
                'message': f'æ•°æ®ç›®å½•å·²æ›´æ–°ä¸º: {self.data_dir.absolute()}',
                'old_dir': str(old_dir),
                'new_dir': str(self.data_dir)
            }
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®ç›®å½•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    

    
    def set_auto_save(self, auto_save: bool):
        """è®¾ç½®è‡ªåŠ¨ä¿å­˜"""
        self.auto_save = auto_save