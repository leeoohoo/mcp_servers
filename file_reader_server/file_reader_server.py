#!/usr/bin/env python3
"""
File Reader MCP Server (æ³¨è§£ç‰ˆæœ¬)
åŸºäº EnhancedMCPServer å’Œè£…é¥°å™¨ç³»ç»Ÿçš„æ–‡ä»¶è¯»å–æœåŠ¡å™¨

ä¸»è¦åŠŸèƒ½:
1. read_file_lines - è¯»å–æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´ï¼ˆæµå¼è¾“å‡ºï¼‰
2. search_files_by_content - æœç´¢æ–‡ä»¶å†…å®¹ï¼ˆæµå¼è¾“å‡ºï¼‰  
3. get_files_content - æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆæµå¼è¾“å‡ºï¼‰
4. get_project_structure - è·å–é¡¹ç›®ç»“æ„ï¼ˆæµå¼è¾“å‡ºï¼‰
"""

import asyncio
import json
import logging
import os
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, AsyncGenerator
from typing_extensions import Annotated

# å¯¼å…¥æ–°çš„æ¡†æ¶
from mcp_framework import (

    MCPHTTPServer,
    ConfigManager,
    setup_logging,
    check_dependencies
)
from mcp_framework.core import EnhancedMCPServer

# å¯¼å…¥è£…é¥°å™¨
from mcp_framework.core.decorators import (
    Required as R,
    Optional as O,
    IntRange,
    ServerParam,
    StringParam,
    BooleanParam,
    PathParam
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger("file_reader_server")


class FileReaderService:
    """æ–‡ä»¶è¯»å–æœåŠ¡ç±»"""

    def __init__(self, project_root: Optional[str] = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()

        # æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
        self.text_extensions = {
            '.txt', '.md', '.py', '.js', '.ts', '.html', '.htm', '.css', '.scss',
            '.json', '.xml', '.yaml', '.yml', '.ini', '.cfg', '.conf', '.log',
            '.sql', '.sh', '.bat', '.ps1', '.php', '.rb', '.go', '.rs', '.cpp',
            '.c', '.h', '.hpp', '.java', '.kt', '.swift', '.dart', '.vue', '.jsx',
            '.tsx', '.svelte', '.astro', '.toml', '.env', '.gitignore', '.dockerfile',
            '.makefile', '.cmake', '.gradle', '.properties', '.csv', '.tsv'
        }

        # é»˜è®¤å¿½ç•¥çš„ç›®å½•
        self.ignore_dirs = {
            '.git', '.svn', '.hg', '__pycache__', 'node_modules', '.vscode',
            '.idea', 'dist', 'build', 'target', '.next', '.nuxt', 'coverage',
            '.pytest_cache', '.mypy_cache', 'venv', 'env', '.env'
        }

        logger.info(f"File Reader Service initialized with project root: {self.project_root}")

    def _should_ignore_path(self, path: Path) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥"""
        # æ£€æŸ¥è·¯å¾„ä¸­çš„ä»»ä½•éƒ¨åˆ†æ˜¯å¦åœ¨å¿½ç•¥åˆ—è¡¨ä¸­
        for part in path.parts:
            if part in self.ignore_dirs:
                return True
        return False

    def _resolve_file_path(self, file_path: str) -> Path:
        """è§£ææ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
        path = Path(file_path)
        if path.is_absolute():
            return path
        else:
            return self.project_root / path

    def _compress_content(self, content: str, show_line_numbers: bool = True) -> str:
        """å‹ç¼©å†…å®¹ï¼Œå»æ‰ç©ºè¡Œå¹¶æ˜¾ç¤ºè¡Œå·"""
        lines = content.split('\n')
        result_lines = []

        for i, line in enumerate(lines, 1):
            if line.strip():  # åªä¿ç•™éç©ºè¡Œ
                if show_line_numbers:
                    result_lines.append(f"{i}:{line}")
                else:
                    result_lines.append(line)

        return '\n'.join(result_lines)

    async def read_file_lines_stream(self, file_path: str, start_line: int, end_line: int) -> AsyncGenerator[str, None]:
        """æµå¼è¯»å–æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´"""
        try:
            # å‚æ•°éªŒè¯
            if not file_path:
                yield json.dumps({"error": "ç¼ºå°‘å¿…è¦å‚æ•° file_path"}, ensure_ascii=False)
                return

            if start_line < 1:
                yield json.dumps({"error": "start_line å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°ï¼ˆ1-basedè¡Œå·ï¼‰"}, ensure_ascii=False)
                return

            if end_line < start_line:
                yield json.dumps({"error": "end_line å¿…é¡»æ˜¯å¤§äºç­‰äº start_line çš„æ•´æ•°"}, ensure_ascii=False)
                return

            # è§£ææ–‡ä»¶è·¯å¾„
            resolved_path = self._resolve_file_path(file_path)

            if not resolved_path.exists():
                yield json.dumps({"error": f"æ–‡ä»¶ä¸å­˜åœ¨ {resolved_path}"}, ensure_ascii=False)
                return

            if not resolved_path.is_file():
                yield json.dumps({"error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶ {resolved_path}"}, ensure_ascii=False)
                return

            # å…ˆå‘é€æ–‡ä»¶ä¿¡æ¯
            yield json.dumps({
                "type": "file_info",
                "file_path": str(resolved_path),
                "request_range": f"{start_line}-{end_line}"
            }, ensure_ascii=False)

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(resolved_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len(all_lines)

            # è°ƒæ•´è¡Œå·èŒƒå›´
            actual_start = max(1, start_line)
            actual_end = min(total_lines, end_line)

            if actual_start > total_lines:
                yield json.dumps({
                    "error": f"èµ·å§‹è¡Œå· {start_line} è¶…å‡ºæ–‡ä»¶æ€»è¡Œæ•° {total_lines}"
                }, ensure_ascii=False)
                return

            # å‘é€æ€»è¡Œæ•°ä¿¡æ¯
            yield json.dumps({
                "type": "meta",
                "total_lines": total_lines,
                "actual_range": f"{actual_start}-{actual_end}"
            }, ensure_ascii=False)

            # æµå¼è¾“å‡ºå†…å®¹
            for i in range(actual_start - 1, actual_end):
                line_content = all_lines[i].rstrip('\n')
                if line_content.strip():  # åªè¾“å‡ºéç©ºè¡Œ
                    yield json.dumps({
                        "type": "content",
                        "line_number": i + 1,
                        "content": line_content
                    }, ensure_ascii=False)

                # æ¯10è¡Œæš‚åœä¸€ä¸‹ï¼Œå…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œ
                if (i + 1) % 10 == 0:
                    await asyncio.sleep(0.01)

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "complete",
                "message": f"æˆåŠŸè¯»å–æ–‡ä»¶ {file_path} ç¬¬ {actual_start}-{actual_end} è¡Œ"
            }, ensure_ascii=False)

        except UnicodeDecodeError:
            yield json.dumps({"error": f"æ–‡ä»¶ç¼–ç ä¸æ”¯æŒï¼Œæ— æ³•è¯»å– {file_path}"}, ensure_ascii=False)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"è¯»å–æ–‡ä»¶å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def search_files_by_content_stream(self, query_text: str, limit: int = 50,
                                             case_sensitive: bool = False, context_lines: int = 20,
                                             file_extensions: Optional[List[str]] = None) -> AsyncGenerator[str, None]:
        """æµå¼æœç´¢æ–‡ä»¶å†…å®¹"""
        try:
            if not query_text:
                yield json.dumps({"error": "ç¼ºå°‘æœç´¢å…³é”®è¯"}, ensure_ascii=False)
                return

            # å‘é€æœç´¢å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "search_start",
                "query": query_text,
                "limit": limit,
                "case_sensitive": case_sensitive
            }, ensure_ascii=False)

            results_count = 0
            search_pattern = re.compile(
                query_text if case_sensitive else query_text,
                0 if case_sensitive else re.IGNORECASE
            )

            # é»˜è®¤æœç´¢çš„æ–‡ä»¶æ‰©å±•å
            if file_extensions is None:
                file_extensions = ['.py', '.kt', '.java', '.js', '.ts', '.cpp', '.c', '.h', '.txt', '.md']

            # éå†é¡¹ç›®ç›®å½•æœç´¢
            for file_path in self.project_root.rglob('*'):
                if not file_path.is_file():
                    continue

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥æ­¤è·¯å¾„
                relative_path = file_path.relative_to(self.project_root)
                if self._should_ignore_path(relative_path):
                    continue

                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if file_extensions:
                    if file_path.suffix not in file_extensions:
                        continue
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ‰©å±•åï¼Œåªå¤„ç†æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶
                    if file_path.suffix not in self.text_extensions:
                        continue

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()

                    # æœç´¢åŒ¹é…è¡Œ
                    for line_num, line in enumerate(lines, 1):
                        if search_pattern.search(line):
                            # è·å–ä¸Šä¸‹æ–‡
                            start_context = max(0, line_num - context_lines - 1)
                            end_context = min(len(lines), line_num + context_lines)

                            context_lines_content = []
                            for i in range(start_context, end_context):
                                line_content = lines[i].rstrip('\n')
                                if line_content.strip():  # åªæ˜¾ç¤ºéç©ºè¡Œ
                                    marker = ">>> " if i == line_num - 1 else "    "
                                    context_lines_content.append(f"{marker}{i + 1}:{line_content}")

                            # æµå¼è¾“å‡ºåŒ¹é…ç»“æœ
                            yield json.dumps({
                                "type": "match",
                                "file": str(file_path.relative_to(self.project_root)),
                                "line": line_num,
                                "context": '\n'.join(context_lines_content)
                            }, ensure_ascii=False)

                            results_count += 1
                            if results_count >= limit:
                                break

                        # æ¯20è¡Œæš‚åœä¸€ä¸‹
                        if line_num % 20 == 0:
                            await asyncio.sleep(0.01)

                    if results_count >= limit:
                        break

                except (UnicodeDecodeError, PermissionError):
                    continue

            # å‘é€æœç´¢å®Œæˆä¿¡å·
            if results_count == 0:
                yield json.dumps({
                    "type": "no_results",
                    "message": f"æœªæ‰¾åˆ°åŒ…å« '{query_text}' çš„æ–‡ä»¶"
                }, ensure_ascii=False)
            else:
                yield json.dumps({
                    "type": "search_complete",
                    "results_count": results_count,
                    "message": f"æ‰¾åˆ° {results_count} ä¸ªåŒ¹é…ç»“æœ"
                }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å†…å®¹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"æœç´¢å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def get_files_content_stream(self, file_paths: List[str]) -> AsyncGenerator[str, None]:
        """æµå¼æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            if not file_paths:
                yield json.dumps({"error": "ç¼ºå°‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨"}, ensure_ascii=False)
                return

            # å‘é€å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "batch_start",
                "total_files": len(file_paths)
            }, ensure_ascii=False)

            for i, file_path in enumerate(file_paths):
                # å‘é€å½“å‰å¤„ç†çš„æ–‡ä»¶ä¿¡æ¯
                yield json.dumps({
                    "type": "file_start",
                    "index": i + 1,
                    "file_path": file_path
                }, ensure_ascii=False)

                resolved_path = self._resolve_file_path(file_path)

                if not resolved_path.exists():
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "æ–‡ä»¶ä¸å­˜åœ¨"
                    }, ensure_ascii=False)
                    continue

                if not resolved_path.is_file():
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "è·¯å¾„ä¸æ˜¯æ–‡ä»¶"
                    }, ensure_ascii=False)
                    continue

                try:
                    with open(resolved_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    compressed_content = self._compress_content(content)
                    total_lines = len(content.split('\n'))

                    # æµå¼è¾“å‡ºæ–‡ä»¶å†…å®¹
                    yield json.dumps({
                        "type": "file_content",
                        "file_path": file_path,
                        "total_lines": total_lines,
                        "content": compressed_content
                    }, ensure_ascii=False)

                except UnicodeDecodeError:
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "æ–‡ä»¶ç¼–ç ä¸æ”¯æŒ"
                    }, ensure_ascii=False)
                except Exception as e:
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": f"è¯»å–å¤±è´¥: {str(e)}"
                    }, ensure_ascii=False)

                # æ¯ä¸ªæ–‡ä»¶å¤„ç†å®Œåæš‚åœ
                await asyncio.sleep(0.01)

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "batch_complete",
                "message": f"æ‰¹é‡è¯»å–å®Œæˆï¼Œå…±å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶"
            }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"æ‰¹é‡è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"æ‰¹é‡è¯»å–å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def get_project_structure_stream(self, max_depth: int = 10, include_hidden: bool = False) -> AsyncGenerator[
        str, None]:
        """æµå¼è·å–é¡¹ç›®ç»“æ„"""
        try:
            # å‘é€å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "structure_start",
                "project_root": str(self.project_root),
                "max_depth": max_depth
            }, ensure_ascii=False)

            async def build_tree_stream(path: Path, prefix: str = "", depth: int = 0):
                if depth > max_depth:
                    return

                try:
                    # è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
                    entries = list(path.iterdir())

                    # è¿‡æ»¤éšè—æ–‡ä»¶å’Œå¿½ç•¥çš„ç›®å½•
                    if not include_hidden:
                        entries = [e for e in entries if not e.name.startswith('.')]

                    # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
                    entries = [e for e in entries if not (e.is_dir() and e.name in self.ignore_dirs)]

                    # æ’åºï¼šç›®å½•åœ¨å‰ï¼Œæ–‡ä»¶åœ¨å
                    entries.sort(key=lambda x: (x.is_file(), x.name.lower()))

                    for i, entry in enumerate(entries):
                        is_last = i == len(entries) - 1
                        current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                        next_prefix = "    " if is_last else "â”‚   "

                        if entry.is_dir():
                            yield json.dumps({
                                "type": "directory",
                                "path": str(entry.relative_to(self.project_root)),
                                "display": f"{prefix}{current_prefix}{entry.name}/",
                                "depth": depth
                            }, ensure_ascii=False)

                            # é€’å½’å¤„ç†å­ç›®å½•
                            async for child_item in build_tree_stream(entry, prefix + next_prefix, depth + 1):
                                yield child_item
                        else:
                            # åªå¯¹æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶è®¡ç®—è¡Œæ•°
                            line_info = ""
                            if entry.suffix in self.text_extensions:
                                try:
                                    with open(entry, 'r', encoding='utf-8') as f:
                                        line_count = sum(1 for _ in f)
                                    line_info = f" ({line_count} lines)"
                                except (UnicodeDecodeError, PermissionError):
                                    line_info = " (no access)"
                                except Exception:
                                    line_info = ""

                            yield json.dumps({
                                "type": "file",
                                "path": str(entry.relative_to(self.project_root)),
                                "display": f"{prefix}{current_prefix}{entry.name}{line_info}",
                                "depth": depth
                            }, ensure_ascii=False)

                        # æ¯10ä¸ªæ¡ç›®æš‚åœä¸€ä¸‹
                        if (i + 1) % 10 == 0:
                            await asyncio.sleep(0.01)

                except PermissionError:
                    yield json.dumps({
                        "type": "error",
                        "path": str(path.relative_to(self.project_root)),
                        "display": f"{prefix}âŒ Permission denied",
                        "depth": depth
                    }, ensure_ascii=False)

            # è¾“å‡ºæ ¹ç›®å½•
            yield json.dumps({
                "type": "root",
                "display": f"ğŸ—ï¸ Project Structure: {self.project_root.name}"
            }, ensure_ascii=False)

            # æµå¼æ„å»ºæ ‘ç»“æ„
            async for item in build_tree_stream(self.project_root):
                yield item

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "structure_complete",
                "message": "é¡¹ç›®ç»“æ„ç”Ÿæˆå®Œæˆ"
            }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"è·å–é¡¹ç›®ç»“æ„æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"è·å–é¡¹ç›®ç»“æ„å¤±è´¥ - {str(e)}"}, ensure_ascii=False)


class FileReaderMCPServer(EnhancedMCPServer):
    """åŸºäºæ³¨è§£è£…é¥°å™¨çš„æ–‡ä»¶è¯»å–MCPæœåŠ¡å™¨"""

    def __init__(self):
        super().__init__(
            name="file-reader-server",
            version="1.0.0",
            description="æ–‡ä»¶è¯»å–MCPæœåŠ¡å™¨ï¼ŒåŸºäºæ³¨è§£è£…é¥°å™¨ç³»ç»Ÿæä¾›æµå¼æ–‡ä»¶æ“ä½œåŠŸèƒ½"
        )
        # åœ¨æ„é€ å‡½æ•°ä¸­å°±åˆå§‹åŒ–æœåŠ¡ï¼Œä½¿ç”¨é»˜è®¤å€¼
        self.file_reader_service = FileReaderService()
        logger.info("FileReaderMCPServer initialized")

    @property
    def setup_tools(self):
        """è®¾ç½®å·¥å…·è£…é¥°å™¨"""

        @self.streaming_tool(description="ğŸ“– **File Line Range Reader** - æµå¼è¯»å–æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´")
        async def read_file_lines(
                file_path: Annotated[str, R("æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹å’Œç»å¯¹è·¯å¾„ï¼‰")],
                start_line: Annotated[int, IntRange("èµ·å§‹è¡Œå·ï¼ˆ1-basedï¼‰", min_val=1)],
                end_line: Annotated[int, IntRange("ç»“æŸè¡Œå·ï¼ˆ1-basedï¼ŒåŒ…å«ï¼‰", min_val=1)]
        ) -> AsyncGenerator[str, None]:
            """æµå¼è¯»å–æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´"""
            async for chunk in self.file_reader_service.read_file_lines_stream(file_path, start_line, end_line):
                yield self._normalize_stream_chunk(chunk)

        @self.streaming_tool(description="ğŸ” **Content Search** - æµå¼æœç´¢æ–‡ä»¶å†…å®¹")
        async def search_files_by_content(
                query_text: Annotated[str, R("æœç´¢å…³é”®è¯")],
                limit: Annotated[int, O("æœ€å¤§ç»“æœæ•°é‡", default=50, minimum=1)] = 50,
                case_sensitive: Annotated[bool, O("æ˜¯å¦åŒºåˆ†å¤§å°å†™", default=False)] = False,
                context_lines: Annotated[int, O("ä¸Šä¸‹æ–‡è¡Œæ•°", default=20, minimum=0)] = 20,
                file_extensions: Annotated[Optional[List[str]], O("æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œå¦‚ ['.py', '.js']")] = None
        ) -> AsyncGenerator[str, None]:
            """æµå¼æœç´¢æ–‡ä»¶å†…å®¹"""
            async for chunk in self.file_reader_service.search_files_by_content_stream(
                    query_text, limit, case_sensitive, context_lines, file_extensions
            ):
                yield self._normalize_stream_chunk(chunk)

        @self.streaming_tool(description="ğŸ“„ **Batch File Reader** - æµå¼æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹")
        async def get_files_content(
                file_paths: Annotated[List[str], R("è¦è¯»å–çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨")]
        ) -> AsyncGenerator[str, None]:
            """æµå¼æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹"""
            async for chunk in self.file_reader_service.get_files_content_stream(file_paths):
                yield self._normalize_stream_chunk(chunk)

        @self.streaming_tool(description="ğŸ—ï¸ **Project Structure** - æµå¼è·å–é¡¹ç›®ç»“æ„")
        async def get_project_structure(
                max_depth: Annotated[int, O("æœ€å¤§éå†æ·±åº¦", default=10, minimum=1)] = 10,
                include_hidden: Annotated[bool, O("æ˜¯å¦åŒ…å«éšè—æ–‡ä»¶", default=False)] = False
        ) -> AsyncGenerator[str, None]:
            """æµå¼è·å–é¡¹ç›®ç»“æ„"""
            async for chunk in self.file_reader_service.get_project_structure_stream(max_depth, include_hidden):
                yield self._normalize_stream_chunk(chunk)

        @self.resource(uri="config://file-reader", name="æ–‡ä»¶è¯»å–å™¨é…ç½®", description="å½“å‰æ–‡ä»¶è¯»å–å™¨çš„é…ç½®ä¿¡æ¯")
        async def file_reader_config_resource(uri: str) -> Dict[str, Any]:
            """è·å–æ–‡ä»¶è¯»å–å™¨é…ç½®ä¿¡æ¯"""
            config_info = {
                "project_root": str(self.file_reader_service.project_root),
                "supported_extensions": list(self.file_reader_service.text_extensions),
                "ignored_directories": list(self.file_reader_service.ignore_dirs),
                "server_config": getattr(self, 'server_config', {})
            }
            return {
                "contents": [
                    {
                        "uri": uri,
                        "mimeType": "application/json",
                        "text": json.dumps(config_info, indent=2, ensure_ascii=False)
                    }
                ]
            }

        @self.resource(uri="stats://project", name="é¡¹ç›®ç»Ÿè®¡", description="é¡¹ç›®æ–‡ä»¶å’Œä»£ç è¡Œæ•°ç»Ÿè®¡")
        async def project_stats_resource(uri: str) -> Dict[str, Any]:
            """è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
            # è®¡ç®—é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
            total_files = 0
            total_lines = 0
            file_types = {}

            try:
                for file_path in self.file_reader_service.project_root.rglob('*'):
                    if not file_path.is_file():
                        continue

                    relative_path = file_path.relative_to(self.file_reader_service.project_root)
                    if self.file_reader_service._should_ignore_path(relative_path):
                        continue

                    total_files += 1
                    ext = file_path.suffix.lower()
                    file_types[ext] = file_types.get(ext, 0) + 1

                    if ext in self.file_reader_service.text_extensions:
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                lines = sum(1 for _ in f)
                                total_lines += lines
                        except (UnicodeDecodeError, PermissionError):
                            pass

                stats_info = {
                    "project_root": str(self.file_reader_service.project_root),
                    "total_files": total_files,
                    "total_lines": total_lines,
                    "file_types": file_types,
                    "text_file_extensions": len(self.file_reader_service.text_extensions)
                }

            except Exception as e:
                stats_info = {"error": f"Failed to calculate stats: {str(e)}"}

            return {
                "contents": [
                    {
                        "uri": uri,
                        "mimeType": "application/json",
                        "text": json.dumps(stats_info, indent=2, ensure_ascii=False)
                    }
                ]
            }

        return True

    @property
    def setup_server_params(self):
        """è®¾ç½®æœåŠ¡å™¨å‚æ•°è£…é¥°å™¨"""

        @self.decorators.server_param("project_root")
        async def project_root_param(
                param: Annotated[str, PathParam(
                    display_name="é¡¹ç›®æ ¹ç›®å½•",
                    description="æœåŠ¡å™¨æ“ä½œçš„æ ¹ç›®å½•è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨å½“å‰ç›®å½•",
                    required=False,
                    placeholder="/path/to/project"
                )]
        ):
            """é¡¹ç›®æ ¹ç›®å½•å‚æ•°"""
            pass

        @self.decorators.server_param("max_file_size")
        async def max_file_size_param(
                param: Annotated[int, ServerParam(
                    display_name="æœ€å¤§æ–‡ä»¶å¤§å° (MB)",
                    description="å…è®¸è¯»å–çš„æœ€å¤§æ–‡ä»¶å¤§å°ï¼Œå•ä½MB",
                    param_type="integer",
                    default_value=10,
                    required=False
                )]
        ):
            """æœ€å¤§æ–‡ä»¶å¤§å°å‚æ•°"""
            pass

        @self.decorators.server_param("enable_hidden_files")
        async def enable_hidden_files_param(
                param: Annotated[bool, BooleanParam(
                    display_name="å¯ç”¨éšè—æ–‡ä»¶",
                    description="æ˜¯å¦å…è®¸è®¿é—®ä»¥ç‚¹(.)å¼€å¤´çš„éšè—æ–‡ä»¶",
                    default_value=False,
                    required=False
                )]
        ):
            """å¯ç”¨éšè—æ–‡ä»¶å‚æ•°"""
            pass

        @self.decorators.server_param("search_limit")
        async def search_limit_param(
                param: Annotated[int, ServerParam(
                    display_name="æœç´¢ç»“æœé™åˆ¶",
                    description="æœç´¢æ“ä½œè¿”å›çš„æœ€å¤§ç»“æœæ•°é‡",
                    param_type="integer",
                    default_value=50,
                    required=False
                )]
        ):
            """æœç´¢ç»“æœé™åˆ¶å‚æ•°"""
            pass

        return True

    async def initialize(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡å™¨ï¼ˆå®ç°åŸºç±»æŠ½è±¡æ–¹æ³•ï¼‰"""
        # è§¦å‘è£…é¥°å™¨å·¥å…·æ³¨å†Œ
        _ = self.setup_tools
        # è§¦å‘æœåŠ¡å™¨å‚æ•°æ³¨å†Œ
        _ = self.setup_server_params

        # è°ƒç”¨åŸºç±»çš„åˆå§‹åŒ–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(super(), 'initialize'):
            await super().initialize()

        # è·å–é…ç½®å‚æ•°å¹¶é‡æ–°åˆå§‹åŒ–æœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        try:
            project_root = self.get_config_value("project_root")
            if project_root and project_root.strip():
                # å¦‚æœé…ç½®äº†é¡¹ç›®æ ¹ç›®å½•ï¼Œé‡æ–°åˆå§‹åŒ–æœåŠ¡
                self.file_reader_service = FileReaderService(project_root.strip())
                logger.info(f"Using configured project root: {project_root}")
            else:
                # ä½¿ç”¨é»˜è®¤çš„å½“å‰ç›®å½•
                logger.info("Using current directory as project root")
        except Exception as e:
            logger.warning(f"Failed to get config, using default: {e}")

        # ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
        if self.file_reader_service is None:
            self.file_reader_service = FileReaderService()
            logger.info("Initialized file reader service with defaults")


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return

    # è®¾ç½®æ—¥å¿—
    setup_logging(log_level=logging.INFO, log_file="file_reader_server.log")

    # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
    mcp_server = FileReaderMCPServer()

    # æ‰‹åŠ¨è°ƒç”¨åˆå§‹åŒ–
    try:
        await mcp_server.initialize()
        print(f"âœ… åˆå§‹åŒ–æˆåŠŸï¼Œå·¥å…·æ•°é‡: {len(mcp_server.tools)}")
        print(f"âœ… èµ„æºæ•°é‡: {len(mcp_server.resources)}")
        for tool in mcp_server.tools:
            print(f"   - {tool['name']}: {tool['description'][:50]}...")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # åŠ è½½é…ç½®
    config_manager = ConfigManager()
    config = config_manager.load_config()

    # åˆ›å»º HTTP æœåŠ¡å™¨
    http_server = MCPHTTPServer(mcp_server, config)

    try:
        # å¯åŠ¨æœåŠ¡å™¨
        runner = await http_server.start()

        print(f"ğŸš€ æ–‡ä»¶è¯»å– MCP æœåŠ¡å™¨å·²å¯åŠ¨!")
        print(f"ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:{config.port}")
        print(f"ğŸ› ï¸  è®¾ç½®é¡µé¢: http://localhost:{config.port}/setup")
        print(f"ğŸ§ª æµ‹è¯•é¡µé¢: http://localhost:{config.port}/test")
        print(f"âš™ï¸  é…ç½®é¡µé¢: http://localhost:{config.port}/config")
        print(f"ğŸ’š å¥åº·æ£€æŸ¥: http://localhost:{config.port}/health")
        print(f"ğŸŒŠ æµå¼API: http://localhost:{config.port}/api/streaming/")
        print()
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")

        # ç­‰å¾…ä¸­æ–­ä¿¡å·
        try:
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")

    except Exception as e:
        logger.error(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
    finally:
        # æ¸…ç†èµ„æº
        try:
            await http_server.stop()
            print("âœ… æœåŠ¡å™¨å·²å®‰å…¨å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æœåŠ¡å™¨æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å†è§!")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        print(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)
