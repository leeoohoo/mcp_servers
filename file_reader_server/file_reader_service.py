#!/usr/bin/env python3
"""
æ–‡ä»¶è¯»å–æœåŠ¡æ¨¡å—
æä¾›æ–‡ä»¶è¯»å–ã€æœç´¢ã€æ‰¹é‡å¤„ç†ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import re
import json
import asyncio
import logging
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, AsyncGenerator

from whoosh.index import create_in, open_dir, exists_in
from whoosh.fields import Schema, TEXT, ID, DATETIME
from whoosh.qparser import QueryParser

# å¯¼å…¥æ–‡ä»¶ç›‘æ§æ¨¡å—
try:
    from file_monitor import RealTimeIndexMonitor
except ImportError:
    RealTimeIndexMonitor = None

logger = logging.getLogger("file_reader_service")


class FileReaderService:
    """æ–‡ä»¶è¯»å–æœåŠ¡ç±»"""

    def __init__(self, project_root: Optional[str] = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        # å°†ç´¢å¼•ç›®å½•æ”¾åœ¨æœåŠ¡å¯åŠ¨ç›®å½•çš„dataç›®å½•ä¸‹ï¼Œä½¿ç”¨é¡¹ç›®æ–‡ä»¶å¤¹åç§°åŒºåˆ†
        service_root = Path(__file__).parent
        data_dir = service_root / "data"
        data_dir.mkdir(exist_ok=True)
        project_name = self.project_root.name
        self.index_dir = data_dir / f"whoosh_index_{project_name}"
        self.monitor = None  # æ–‡ä»¶ç›‘æ§å™¨

        # æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
        self.text_extensions = {
            '.txt', '.md', '.py', '.js', '.ts', '.html', '.htm', '.css', '.scss',
            '.json', '.xml', '.yaml', '.yml', '.ini', '.cfg', '.conf', '.log',
            '.sql', '.sh', '.bat', '.ps1', '.php', '.rb', '.go', '.rs', '.cpp',
            '.c', '.h', '.hpp', '.java', '.kt', '.swift', '.dart', '.vue', '.jsx',
            '.tsx', '.svelte', '.astro', '.toml', '.env', '.gitignore', '.dockerfile',
            '.makefile', '.cmake', '.gradle', '.properties', '.csv', '.tsv'
        }

        # é»˜è®¤å¿½ç•¥çš„ç›®å½•
        self.ignore_dirs = {
            '.git', '.svn', '.hg', '__pycache__', 'node_modules', '.vscode',
            '.idea', 'dist', 'build', 'target', '.next', '.nuxt', 'coverage',
            '.pytest_cache', '.mypy_cache', 'venv', 'env', '.env', 'whoosh_index'
        }

        # åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºç´¢å¼•
        self._ensure_index_exists()
        
        # åˆå§‹åŒ–ç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰å¹¶é»˜è®¤å¯åŠ¨
        if RealTimeIndexMonitor:
            self.monitor = RealTimeIndexMonitor(str(self.project_root), str(self.index_dir))
            # é»˜è®¤å¯åŠ¨æ–‡ä»¶ç›‘æ§
            try:
                self.monitor.start_monitoring()
                logger.info("æ–‡ä»¶ç›‘æ§å·²é»˜è®¤å¯åŠ¨")
            except Exception as e:
                logger.error(f"å¯åŠ¨æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
        
        logger.info(f"File Reader Service initialized with project root: {self.project_root}")

    def _should_ignore_path(self, path: Path) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥"""
        # æ£€æŸ¥è·¯å¾„ä¸­çš„ä»»ä½•éƒ¨åˆ†æ˜¯å¦åœ¨å¿½ç•¥åˆ—è¡¨ä¸­
        for part in path.parts:
            if part in self.ignore_dirs:
                return True
            # ç‰¹æ®Šå¤„ç†ï¼šå¿½ç•¥æ‰€æœ‰ä»¥whoosh_indexå¼€å¤´çš„ç›®å½•
            if part.startswith('whoosh_index'):
                return True
        return False

    def _resolve_file_path(self, file_path: str) -> Path:
        """è§£ææ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
        path = Path(file_path)
        if path.is_absolute():
            return path
        else:
            return self.project_root / path

    def _compress_content(self, content: str, show_line_numbers: bool = True) -> str:
        """å‹ç¼©å†…å®¹ï¼Œå»æ‰ç©ºè¡Œå¹¶æ˜¾ç¤ºè¡Œå·"""
        lines = content.split('\n')
        result_lines = []

        for i, line in enumerate(lines, 1):
            if line.strip():  # åªä¿ç•™éç©ºè¡Œ
                if show_line_numbers:
                    result_lines.append(f"{i}:{line}")
                else:
                    result_lines.append(line)

        return '\n'.join(result_lines)

    def _ensure_index_exists(self):
        """ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not self.index_dir.exists():
            self.index_dir.mkdir(parents=True)
            
        if not exists_in(str(self.index_dir)):
            logger.info("ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºæ–°ç´¢å¼•...")
            self._create_or_update_index()
        else:
            logger.info("ç´¢å¼•å·²å­˜åœ¨")
    
    def _create_or_update_index(self):
        """åˆ›å»ºæˆ–æ›´æ–°Whooshç´¢å¼•"""
        schema = Schema(
            path=ID(stored=True, unique=True), 
            content=TEXT(stored=True),
            modified_time=DATETIME(stored=True),
            file_hash=ID(stored=True)
        )
        
        if not self.index_dir.exists():
            self.index_dir.mkdir(parents=True)
            
        if exists_in(str(self.index_dir)):
            ix = open_dir(str(self.index_dir))
        else:
            ix = create_in(str(self.index_dir), schema)
            
        writer = ix.writer()
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        writer.delete_by_term('path', '*')
        
        for file_path in self.project_root.rglob('*'):
            if not file_path.is_file():
                continue
                
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥æ­¤è·¯å¾„
            relative_path = file_path.relative_to(self.project_root)
            if self._should_ignore_path(relative_path):
                continue
                
            # åªå¤„ç†æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶
            if file_path.suffix in self.text_extensions:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå’Œä¿®æ”¹æ—¶é—´
                    mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                    file_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                    
                    writer.add_document(
                        path=str(file_path), 
                        content=content,
                        modified_time=mtime,
                        file_hash=file_hash
                    )
                except (UnicodeDecodeError, PermissionError):
                    continue
                    
        writer.commit()
        logger.info("Whooshç´¢å¼•æ„å»ºå®Œæˆ")
        
    def _get_line_numbers(self, file_path: str, keyword: str, max_matches: int = 10):
        """ä»æ–‡ä»¶ä¸­æå–åŒ¹é…å…³é”®è¯çš„è¡Œå·å’Œå†…å®¹ï¼ŒåŒæ—¶è¿”å›æ–‡ä»¶æ€»è¡Œæ•°"""
        matches = []
        total_lines = 0
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, start=1):
                    total_lines = line_num  # è®°å½•æ€»è¡Œæ•°
                    if pattern.search(line):
                        # é«˜äº®è¡Œå†…å®¹
                        highlighted_line = pattern.sub(lambda m: f"**{m.group(0)}**", line.strip())
                        matches.append({
                            "line_number": line_num,
                            "content": highlighted_line
                        })
                    if len(matches) >= max_matches:
                        # å³ä½¿æ‰¾åˆ°è¶³å¤Ÿçš„åŒ¹é…ï¼Œä¹Ÿè¦ç»§ç»­è¯»å–ä»¥è·å–æ€»è¡Œæ•°
                        for remaining_line in f:
                            total_lines += 1
                        break
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {file_path} å‡ºé”™: {e}")
            
        return matches, total_lines

    async def read_file_lines_stream(self, file_path: str, start_line: int, end_line: int) -> AsyncGenerator[str, None]:
        """æµå¼è¯»å–æ–‡ä»¶æŒ‡å®šè¡ŒèŒƒå›´"""
        try:
            # å‚æ•°éªŒè¯
            if not file_path:
                yield json.dumps({"error": "ç¼ºå°‘å¿…è¦å‚æ•° file_path"}, ensure_ascii=False)
                return

            if start_line < 1:
                yield json.dumps({"error": "start_line å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°ï¼ˆ1-basedè¡Œå·ï¼‰"}, ensure_ascii=False)
                return

            if end_line < start_line:
                yield json.dumps({"error": "end_line å¿…é¡»æ˜¯å¤§äºç­‰äº start_line çš„æ•´æ•°"}, ensure_ascii=False)
                return

            # è§£ææ–‡ä»¶è·¯å¾„
            resolved_path = self._resolve_file_path(file_path)

            if not resolved_path.exists():
                yield json.dumps({"error": f"æ–‡ä»¶ä¸å­˜åœ¨ {resolved_path}"}, ensure_ascii=False)
                return

            if not resolved_path.is_file():
                yield json.dumps({"error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶ {resolved_path}"}, ensure_ascii=False)
                return

            # å…ˆå‘é€æ–‡ä»¶ä¿¡æ¯
            yield json.dumps({
                "type": "file_info",
                "file_path": str(resolved_path),
                "request_range": f"{start_line}-{end_line}"
            }, ensure_ascii=False)

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(resolved_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len(all_lines)

            # è°ƒæ•´è¡Œå·èŒƒå›´
            actual_start = max(1, start_line)
            actual_end = min(total_lines, end_line)

            if actual_start > total_lines:
                yield json.dumps({
                    "error": f"èµ·å§‹è¡Œå· {start_line} è¶…å‡ºæ–‡ä»¶æ€»è¡Œæ•° {total_lines}"
                }, ensure_ascii=False)
                return

            # å‘é€æ€»è¡Œæ•°ä¿¡æ¯
            yield json.dumps({
                "type": "meta",
                "total_lines": total_lines,
                "actual_range": f"{actual_start}-{actual_end}"
            }, ensure_ascii=False)

            # æµå¼è¾“å‡ºå†…å®¹
            for i in range(actual_start - 1, actual_end):
                line_content = all_lines[i].rstrip('\n')
                if line_content.strip():  # åªè¾“å‡ºéç©ºè¡Œ
                    yield json.dumps({
                        "type": "content",
                        "line_number": i + 1,
                        "content": line_content
                    }, ensure_ascii=False)

                # æ¯10è¡Œæš‚åœä¸€ä¸‹ï¼Œå…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œ
                if (i + 1) % 10 == 0:
                    await asyncio.sleep(0.01)

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "complete",
                "message": f"æˆåŠŸè¯»å–æ–‡ä»¶ {file_path} ç¬¬ {actual_start}-{actual_end} è¡Œ"
            }, ensure_ascii=False)

        except UnicodeDecodeError:
            yield json.dumps({"error": f"æ–‡ä»¶ç¼–ç ä¸æ”¯æŒï¼Œæ— æ³•è¯»å– {file_path}"}, ensure_ascii=False)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"è¯»å–æ–‡ä»¶å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def search_files_by_content_stream(self, query_text: str, limit: int = 50,
                                             case_sensitive: bool = False, context_lines: int = 20,
                                             file_extensions: Optional[List[str]] = None) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Whooshè¿›è¡Œæµå¼æœç´¢æ–‡ä»¶å†…å®¹ï¼Œåªè¿”å›æ–‡ä»¶åœ°å€å’ŒåŒ¹é…è¡Œè¯¦æƒ…"""
        try:
            if not query_text:
                yield json.dumps({"error": "ç¼ºå°‘æœç´¢å…³é”®è¯"}, ensure_ascii=False)
                return

            # å‘é€æœç´¢å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "search_start",
                "query": query_text,
                "limit": limit,
                "case_sensitive": case_sensitive,
                "message": "å¼€å§‹æœç´¢..."
            }, ensure_ascii=False)

            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            if not exists_in(str(self.index_dir)):
                yield json.dumps({
                    "type": "index_creating",
                    "message": "ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç´¢å¼•..."
                }, ensure_ascii=False)
                self._create_or_update_index()
                await asyncio.sleep(0.01)  # è®©å‡ºæ§åˆ¶æƒ
                
                yield json.dumps({
                    "type": "index_complete",
                    "message": "ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå¼€å§‹æœç´¢..."
                }, ensure_ascii=False)

            # ä½¿ç”¨Whooshè¿›è¡Œæœç´¢
            ix = open_dir(str(self.index_dir))
            results_count = 0
            max_lines_per_file = 10  # æ¯ä¸ªæ–‡ä»¶æœ€å¤šè¿”å›çš„åŒ¹é…è¡Œæ•°

            with ix.searcher() as searcher:
                query = QueryParser("content", ix.schema).parse(query_text)
                results = searcher.search(query, limit=limit)

                for hit in results:
                    file_path = hit['path']
                    
                    # è·å–åŒ¹é…è¡Œçš„è¯¦ç»†ä¿¡æ¯å’Œæ–‡ä»¶æ€»è¡Œæ•°
                    line_matches, total_lines = self._get_line_numbers(file_path, query_text, max_matches=max_lines_per_file)
                    
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„
                    try:
                        relative_path = str(Path(file_path).relative_to(self.project_root))
                    except ValueError:
                        relative_path = file_path

                    # æµå¼è¾“å‡ºåŒ¹é…ç»“æœï¼ˆåŒ…å«æ–‡ä»¶åœ°å€ã€åŒ¹é…è¡Œè¯¦æƒ…å’Œæ–‡ä»¶æ€»è¡Œæ•°ï¼‰
                    yield json.dumps({
                        "type": "match",
                        "file_path": relative_path,
                        "line_matches": line_matches,
                        "total_matches_in_file": len(line_matches),
                        "total_lines": total_lines
                    }, ensure_ascii=False)

                    results_count += 1
                    
                    # æ¯å¤„ç†ä¸€ä¸ªæ–‡ä»¶æš‚åœä¸€ä¸‹
                    await asyncio.sleep(0.01)

            # å‘é€æœç´¢å®Œæˆä¿¡å·
            if results_count == 0:
                yield json.dumps({
                    "type": "no_results",
                    "message": f"æœªæ‰¾åˆ°åŒ…å« '{query_text}' çš„æ–‡ä»¶"
                }, ensure_ascii=False)
            else:
                yield json.dumps({
                    "type": "search_complete",
                    "results_count": results_count,
                    "message": f"æ‰¾åˆ° {results_count} ä¸ªåŒ¹é…æ–‡ä»¶"
                }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å†…å®¹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"æœç´¢å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def get_files_content_stream(self, file_paths: List[str]) -> AsyncGenerator[str, None]:
        """æµå¼æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            if not file_paths:
                yield json.dumps({"error": "ç¼ºå°‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨"}, ensure_ascii=False)
                return

            # å‘é€å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "batch_start",
                "total_files": len(file_paths)
            }, ensure_ascii=False)

            for i, file_path in enumerate(file_paths):
                # å‘é€å½“å‰å¤„ç†çš„æ–‡ä»¶ä¿¡æ¯
                yield json.dumps({
                    "type": "file_start",
                    "index": i + 1,
                    "file_path": file_path
                }, ensure_ascii=False)

                resolved_path = self._resolve_file_path(file_path)

                if not resolved_path.exists():
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "æ–‡ä»¶ä¸å­˜åœ¨"
                    }, ensure_ascii=False)
                    continue

                if not resolved_path.is_file():
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "è·¯å¾„ä¸æ˜¯æ–‡ä»¶"
                    }, ensure_ascii=False)
                    continue

                try:
                    with open(resolved_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    compressed_content = self._compress_content(content)
                    total_lines = len(content.split('\n'))

                    # æµå¼è¾“å‡ºæ–‡ä»¶å†…å®¹
                    yield json.dumps({
                        "type": "file_content",
                        "file_path": file_path,
                        "total_lines": total_lines,
                        "content": compressed_content
                    }, ensure_ascii=False)

                except UnicodeDecodeError:
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": "æ–‡ä»¶ç¼–ç ä¸æ”¯æŒ"
                    }, ensure_ascii=False)
                except Exception as e:
                    yield json.dumps({
                        "type": "file_error",
                        "file_path": file_path,
                        "error": f"è¯»å–å¤±è´¥: {str(e)}"
                    }, ensure_ascii=False)

                # æ¯ä¸ªæ–‡ä»¶å¤„ç†å®Œåæš‚åœ
                await asyncio.sleep(0.01)

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "batch_complete",
                "message": f"æ‰¹é‡è¯»å–å®Œæˆï¼Œå…±å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶"
            }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"æ‰¹é‡è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"æ‰¹é‡è¯»å–å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    async def get_project_structure_stream(self, max_depth: int = 10, include_hidden: bool = False) -> AsyncGenerator[
        str, None]:
        """æµå¼è·å–é¡¹ç›®ç»“æ„"""
        try:
            # å‘é€å¼€å§‹ä¿¡å·
            yield json.dumps({
                "type": "structure_start",
                "project_root": str(self.project_root),
                "max_depth": max_depth
            }, ensure_ascii=False)

            async def build_tree_stream(path: Path, prefix: str = "", depth: int = 0):
                if depth > max_depth:
                    return

                try:
                    # è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
                    entries = list(path.iterdir())

                    # è¿‡æ»¤éšè—æ–‡ä»¶å’Œå¿½ç•¥çš„ç›®å½•
                    if not include_hidden:
                        entries = [e for e in entries if not e.name.startswith('.')]

                    # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
                    entries = [e for e in entries if not (e.is_dir() and e.name in self.ignore_dirs)]

                    # æ’åºï¼šç›®å½•åœ¨å‰ï¼Œæ–‡ä»¶åœ¨å
                    entries.sort(key=lambda x: (x.is_file(), x.name.lower()))

                    for i, entry in enumerate(entries):
                        is_last = i == len(entries) - 1
                        current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                        next_prefix = "    " if is_last else "â”‚   "

                        if entry.is_dir():
                            yield json.dumps({
                                "type": "directory",
                                "path": str(entry.relative_to(self.project_root)),
                                "display": f"{prefix}{current_prefix}{entry.name}/",
                                "depth": depth
                            }, ensure_ascii=False)

                            # é€’å½’å¤„ç†å­ç›®å½•
                            async for child_item in build_tree_stream(entry, prefix + next_prefix, depth + 1):
                                yield child_item
                        else:
                            # åªå¯¹æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶è®¡ç®—è¡Œæ•°
                            line_info = ""
                            if entry.suffix in self.text_extensions:
                                try:
                                    with open(entry, 'r', encoding='utf-8') as f:
                                        line_count = sum(1 for _ in f)
                                    line_info = f" ({line_count} lines)"
                                except (UnicodeDecodeError, PermissionError):
                                    line_info = " (no access)"
                                except Exception:
                                    line_info = ""

                            yield json.dumps({
                                "type": "file",
                                "path": str(entry.relative_to(self.project_root)),
                                "display": f"{prefix}{current_prefix}{entry.name}{line_info}",
                                "depth": depth
                            }, ensure_ascii=False)

                        # æ¯10ä¸ªæ¡ç›®æš‚åœä¸€ä¸‹
                        if (i + 1) % 10 == 0:
                            await asyncio.sleep(0.01)

                except PermissionError:
                    yield json.dumps({
                        "type": "error",
                        "path": str(path.relative_to(self.project_root)),
                        "display": f"{prefix}âŒ Permission denied",
                        "depth": depth
                    }, ensure_ascii=False)

            # è¾“å‡ºæ ¹ç›®å½•
            yield json.dumps({
                "type": "root",
                "display": f"ğŸ—ï¸ Project Structure: {self.project_root.name}"
            }, ensure_ascii=False)

            # æµå¼æ„å»ºæ ‘ç»“æ„
            async for item in build_tree_stream(self.project_root):
                yield item

            # å‘é€å®Œæˆä¿¡å·
            yield json.dumps({
                "type": "structure_complete",
                "message": "é¡¹ç›®ç»“æ„ç”Ÿæˆå®Œæˆ"
            }, ensure_ascii=False)

        except Exception as e:
            logger.info(f"è·å–é¡¹ç›®ç»“æ„æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            yield json.dumps({"error": f"è·å–é¡¹ç›®ç»“æ„å¤±è´¥ - {str(e)}"}, ensure_ascii=False)

    def start_monitoring(self) -> Dict[str, Any]:
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§"""
        if not self.monitor:
            if RealTimeIndexMonitor:
                self.monitor = RealTimeIndexMonitor(str(self.project_root), str(self.index_dir))
            else:
                return {"error": "æ–‡ä»¶ç›‘æ§æ¨¡å—ä¸å¯ç”¨"}
        
        if self.monitor.is_running():
            return {"message": "ç›‘æ§å·²ç»åœ¨è¿è¡Œä¸­"}
        
        try:
            self.monitor.start_monitoring()
            return {"success": True, "message": "æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨"}
        except Exception as e:
            logger.error(f"å¯åŠ¨æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
            return {"error": f"å¯åŠ¨ç›‘æ§å¤±è´¥: {str(e)}"}
    
    def stop_monitoring(self) -> Dict[str, Any]:
        """åœæ­¢æ–‡ä»¶ç›‘æ§"""
        if not self.monitor:
            return {"message": "ç›‘æ§å™¨æœªåˆå§‹åŒ–"}
        
        if not self.monitor.is_running():
            return {"message": "ç›‘æ§æœªåœ¨è¿è¡Œ"}
        
        try:
            self.monitor.stop_monitoring()
            return {"success": True, "message": "æ–‡ä»¶ç›‘æ§å·²åœæ­¢"}
        except Exception as e:
            logger.error(f"åœæ­¢æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
            return {"error": f"åœæ­¢ç›‘æ§å¤±è´¥: {str(e)}"}
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§çŠ¶æ€"""
        if not self.monitor:
            return {
                "available": RealTimeIndexMonitor is not None,
                "running": False,
                "message": "ç›‘æ§å™¨æœªåˆå§‹åŒ–"
            }
        
        return {
            "available": True,
            "running": self.monitor.is_running(),
            "project_root": str(self.project_root),
            "index_dir": str(self.index_dir)
        }
    
    def update_project_root(self, new_project_root: str) -> Dict[str, Any]:
        """åŠ¨æ€æ›´æ–°é¡¹ç›®æ ¹ç›®å½•"""
        try:
            # åœæ­¢å½“å‰ç›‘æ§ï¼ˆå¦‚æœæ­£åœ¨è¿è¡Œï¼‰
            if self.monitor and self.monitor.is_running():
                self.stop_monitoring()
            
            # æ›´æ–°é¡¹ç›®æ ¹ç›®å½•
            old_root = str(self.project_root)
            self.project_root = Path(new_project_root) if new_project_root else Path.cwd()
            
            # æ›´æ–°ç´¢å¼•ç›®å½•ï¼Œæ”¾åœ¨æœåŠ¡å¯åŠ¨ç›®å½•çš„dataç›®å½•ä¸‹
            service_root = Path(__file__).parent
            data_dir = service_root / "data"
            data_dir.mkdir(exist_ok=True)
            project_name = self.project_root.name
            self.index_dir = data_dir / f"whoosh_index_{project_name}"
            
            # é‡æ–°åˆ›å»ºç´¢å¼•
            self._ensure_index_exists()
            
            # é‡æ–°åˆå§‹åŒ–ç›‘æ§å™¨
            if RealTimeIndexMonitor:
                self.monitor = RealTimeIndexMonitor(str(self.project_root), str(self.index_dir))
            
            logger.info(f"Project root updated from {old_root} to {self.project_root}")
            
            return {
                "success": True,
                "message": f"é¡¹ç›®æ ¹ç›®å½•å·²æ›´æ–°ä¸º: {self.project_root}",
                "old_root": old_root,
                "new_root": str(self.project_root)
            }
            
        except Exception as e:
            logger.error(f"æ›´æ–°é¡¹ç›®æ ¹ç›®å½•å¤±è´¥: {e}")
            return {
                "error": f"æ›´æ–°å¤±è´¥: {str(e)}"
            }