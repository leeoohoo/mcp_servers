import json
import logging

from typing import Any, Dict, List, Optional, AsyncGenerator

# ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼Œä½†ä¸åŒ…å«åŒ…åå‰ç¼€
from ai_request_handler import AiRequestHandler
from mcp_tool_execute import McpToolExecute

# é…ç½®æ—¥å¿—
logger = logging.getLogger("AiClient")


class AiClient:
    """AIå®¢æˆ·ç«¯"""

    def __init__(self, messages: List[Dict[str, Any]], conversation_id: str,
                 tools: List[Dict[str, Any]], model_config: Dict[str, Any],
                 callback, mcp_tool_execute: McpToolExecute):
        self.messages = messages
        self.conversation_id = conversation_id
        self.tools = tools
        self.model_config = model_config
        self.callback = callback
        self.mcp_tool_execute = mcp_tool_execute
        self.is_aborted = False
        self.current_ai_request_handler = None

    # éæµå¼æ–¹æ³•å·²ç§»é™¤ï¼Œåªä¿ç•™æµå¼ç‰ˆæœ¬

    async def start_stream(self) -> AsyncGenerator[str, None]:
        """å¼€å§‹æµå¼AIå¯¹è¯"""
        try:
            if self.callback:
                self.callback("conversation_start", {"conversation_id": self.conversation_id})

            async for chunk in self.handle_tool_call_recursively_stream(max_rounds=25, current_round=0):
                yield chunk

            if self.callback:
                self.callback("conversation_end", {"conversation_id": self.conversation_id})

        except Exception as e:
            if self.callback:
                self.callback("error", {"error": str(e)})
            yield json.dumps({"type": "error", "data": str(e)}, ensure_ascii=False)

    async def handle_tool_call_recursively_stream(self, max_rounds: int, current_round: int) -> AsyncGenerator[
        str, None]:
        """é€’å½’å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        logger.info(f"ğŸŒŠ å¼€å§‹ç¬¬ {current_round + 1} è½®æµå¼AIå¯¹è¯å¤„ç†")

        if current_round >= max_rounds:
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶ ({max_rounds})")
            return

        if self.is_aborted:
            logger.info('ğŸ›‘ Request aborted')
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
        has_pending_tools = (self.messages and
                             self.messages[-1].get('role') == 'assistant' and
                             self.messages[-1].get('tool_calls', []))

        if has_pending_tools:
            logger.info(f"ğŸ”§ ç¬¬ {current_round + 1} è½®ï¼šå‘ç°å¾…æ‰§è¡Œçš„å·¥å…·è°ƒç”¨")

            # ğŸŒŠ æµå¼æ‰§è¡Œå·¥å…·å¹¶å®æ—¶yieldå†…å®¹
            async for chunk in self._execute_pending_tool_calls_stream():
                yield chunk

            # å·¥å…·æ‰§è¡Œå®Œæˆåï¼Œè¿›å…¥ä¸‹ä¸€è½®é€’å½’
            async for chunk in self.handle_tool_call_recursively_stream(max_rounds, current_round + 1):
                yield chunk

        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿›è¡ŒAIèŠå¤©
            async for chunk in self.chat_completion_stream():
                yield chunk

            # æ£€æŸ¥AIå“åº”åæ˜¯å¦æœ‰æ–°çš„å·¥å…·è°ƒç”¨
            has_new_tools = (self.messages and
                             self.messages[-1].get('role') == 'assistant' and
                             self.messages[-1].get('tool_calls', []))

            if has_new_tools:
                async for chunk in self.handle_tool_call_recursively_stream(max_rounds, current_round + 1):
                    yield chunk

    async def _execute_pending_tool_calls_stream(self) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œå¾…å¤„ç†çš„å·¥å…·è°ƒç”¨ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        if not self.messages:
            return

        latest_message = self.messages[-1]
        tool_calls = latest_message.get('tool_calls', [])

        if not tool_calls:
            return

        logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

        if self.callback:
            self.callback('tool_call', tool_calls)

        # æ£€æŸ¥åœæ­¢å·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            if not isinstance(tool_call, dict):
                continue
            tool_name = tool_call.get('function', {}).get('name') or tool_call.get('name')
            if tool_name == 'stop_conversation':
                logger.info(f"ğŸ›‘ æ”¶åˆ°åœæ­¢å¯¹è¯è¯·æ±‚")
                if self.callback:
                    self.callback('conversation_stopped', {'reason': 'ç”¨æˆ·è¯·æ±‚åœæ­¢'})
                return

        # ç´¯ç§¯æ¯ä¸ªå·¥å…·çš„å®Œæ•´å†…å®¹
        tool_contents = {}  # tool_call_id -> ç´¯ç§¯çš„å†…å®¹
        tool_results = []

        # æ‰§è¡Œå¹¶ç´¯ç§¯æ‰€æœ‰æµå¼å†…å®¹
        async for tool_result in self.mcp_tool_execute.execute_stream(tool_calls, self.callback):
            tool_call_id = tool_result.get('tool_call_id')
            tool_name = tool_result.get('name')
            content = tool_result.get('content', '')
            is_final = tool_result.get('is_final', False)

            logger.info(
                f"ğŸ”§ æ”¶åˆ°å·¥å…·ç»“æœ: tool_call_id={tool_call_id}, name={tool_name}, content_len={len(content)}, is_final={is_final}")

            if not tool_call_id:
                logger.warning(f"âš ï¸ å·¥å…·ç»“æœç¼ºå°‘ tool_call_id")
                continue

            # ğŸŒŠ å®æ—¶yieldå·¥å…·æ‰§è¡Œçš„æµå¼å†…å®¹
            if content and not is_final:
                # æ ¼å¼åŒ–å·¥å…·è¾“å‡º
                tool_chunk = json.dumps({
                    "type": "tool_stream",
                    "tool_name": tool_name,
                    "tool_call_id": tool_call_id,
                    "content": content
                }, ensure_ascii=False)
                yield tool_chunk

            # ç´¯ç§¯å†…å®¹
            if tool_call_id not in tool_contents:
                tool_contents[tool_call_id] = ""

            if not is_final:
                tool_contents[tool_call_id] += content
            else:
                # æœ€ç»ˆç»“æœ
                final_content = tool_contents[tool_call_id] + content if tool_contents[tool_call_id] else content

                complete_result = {
                    'tool_call_id': tool_call_id,
                    'role': 'tool',
                    'name': tool_name,
                    'content': final_content
                }
                tool_results.append(complete_result)

        # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
        logger.info(f"ğŸ”§ æ·»åŠ  {len(tool_results)} ä¸ªå·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²")
        self.messages.extend(tool_results)

        if self.callback:
            self.callback('tool_result', tool_results)

        # å‘é€å·¥å…·æ‰§è¡Œå®Œæˆä¿¡å·
        completion_signal = json.dumps({
            "type": "tool_complete",
            "tool_count": len(tool_results)
        }, ensure_ascii=False)
        yield completion_signal

    async def chat_completion_stream(self) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        try:
            logger.info(f"ğŸŒŠ åˆ›å»ºæµå¼AIè¯·æ±‚å¤„ç†å™¨ï¼Œæ¶ˆæ¯æ•°: {len(self.messages)}, å·¥å…·æ•°: {len(self.tools)}")
            self.current_ai_request_handler = AiRequestHandler(
                self.messages, self.tools, self.conversation_id,
                self.callback, self.model_config
            )

            logger.info(f"ğŸŒŠ å¼€å§‹æ‰§è¡Œæµå¼èŠå¤©å®Œæˆè¯·æ±‚")
            chunk_count = 0
            async for chunk in self.current_ai_request_handler.chat_completion_stream():
                chunk_count += 1
                logger.debug(f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk[:50]}..." if len(
                    chunk) > 50 else f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk}")
                yield chunk

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†AiRequestHandlerçš„æ¶ˆæ¯å†å²åŒæ­¥å›AiClient
            # AiRequestHandleråœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¼šå°†AIå“åº”æ·»åŠ åˆ°å…¶æœ¬åœ°messagesä¸­
            # æˆ‘ä»¬éœ€è¦å°†è¿™äº›æ›´æ–°åŒæ­¥å›AiClientçš„messages
            logger.info(f"ğŸ”§ åŒæ­¥æ¶ˆæ¯å†å²ï¼šAiRequestHandleræ¶ˆæ¯æ•° {len(self.current_ai_request_handler.messages)}, AiClientæ¶ˆæ¯æ•° {len(self.messages)}")
            
            # å¦‚æœAiRequestHandlerçš„æ¶ˆæ¯æ•°é‡æ›´å¤šï¼Œè¯´æ˜æœ‰æ–°çš„AIå“åº”è¢«æ·»åŠ 
            if len(self.current_ai_request_handler.messages) > len(self.messages):
                # è·å–æ–°æ·»åŠ çš„æ¶ˆæ¯
                new_messages = self.current_ai_request_handler.messages[len(self.messages):]
                self.messages.extend(new_messages)
                logger.info(f"ğŸ”§ å·²åŒæ­¥ {len(new_messages)} æ¡æ–°æ¶ˆæ¯åˆ°AiClientï¼Œå½“å‰æ€»æ¶ˆæ¯æ•°: {len(self.messages)}")
                
                # æ‰“å°æœ€æ–°æ¶ˆæ¯çš„ä¿¡æ¯
                if new_messages:
                    latest_msg = new_messages[-1]
                    logger.info(f"ğŸ”§ æœ€æ–°åŒæ­¥çš„æ¶ˆæ¯è§’è‰²: {latest_msg.get('role')}, å·¥å…·è°ƒç”¨æ•°: {len(latest_msg.get('tool_calls', []))}")

            logger.info(f"âœ… æµå¼èŠå¤©å®Œæˆè¯·æ±‚æ‰§è¡ŒæˆåŠŸï¼Œæ€»å…±äº§ç”Ÿ {chunk_count} ä¸ªå—")

        except Exception as e:
            logger.error(f"âŒ Stream chat completion error: {e}")
            if self.callback:
                self.callback('error', e)
            yield json.dumps({"type": "error", "data": str(e)}, ensure_ascii=False)

    def abort(self):
        """ä¸­æ­¢è¯·æ±‚"""
        self.is_aborted = True
        if self.current_ai_request_handler:
            self.current_ai_request_handler.abort()
