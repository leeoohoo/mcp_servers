import json
import logging
from typing import Any, Dict, AsyncGenerator
from typing_extensions import Annotated

from expert_service import ExpertService
from mcp_framework.core import EnhancedMCPServer
from mcp_framework.core.decorators import (
    Required as R,
    ServerParam, StringParam, BooleanParam, SelectParam
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger("ExpertMCPServerAnnotated")


class ExpertMCPServerAnnotated(EnhancedMCPServer):
    """åŸºäºæ³¨è§£è£…é¥°å™¨çš„ä¸“å®¶MCPæœåŠ¡å™¨"""

    def __init__(self):
        super().__init__(
            name="expert-server-annotated",
            version="1.0.0",
            description="AIä¸“å®¶æœåŠ¡å™¨ï¼ŒåŸºäºæ³¨è§£è£…é¥°å™¨ç³»ç»Ÿæä¾›å®Œæ•´çš„AIå·¥å…·è°ƒç”¨ä½“ç³»"
        )
        self.expert_service = None

    @property
    def setup_tools(self):
        """è®¾ç½®å·¥å…·è£…é¥°å™¨"""

        # @self.streaming_tool(
        #     description="**Development Planner** - Analyzes project status and creates development execution plans\n\n" +
        #                 "**Input Parameter**: question (string) - Development requirements or objectives\n\n" +
        #                 "**Planning Capabilities**:\n" +
        #                 "â€¢ Scan project structure and existing codebase\n" +
        #                 "â€¢ Identify tech stack and development patterns\n" +
        #                 "â€¢ Create detailed development plans\n" +
        #                 "â€¢ Plan task execution order and dependencies\n" +
        #                 "â€¢ Estimate development effort and timeline\n\n" +
        #                 "**Output**: Structured development plan containing:\n" +
        #                 "- Project analysis results\n" +
        #                 "- Priority-ordered task list\n" +
        #                 "- Specific execution instructions for each task\n" +
        #                 "- File paths and operation types\n" +
        #                 "- Inter-task dependencies\n\n" +
        #                 "**Usage**: query_expert_stream(question=\"Feature to implement or problem to solve\")"
        # )
        # async def query_expert_stream(
        #         question: Annotated[str, R(
        #             "Describe the development objective to be planned, for example:\n" +
        #             "â€¢ 'Implement user management system'\n" +
        #             "â€¢ 'Refactor existing code architecture'\n" +
        #             "â€¢ 'Add payment functionality module'\n" +
        #             "â€¢ 'Optimize system performance'\n" +
        #             "â€¢ 'Fix known bug list'"
        #         )]
        # ) -> AsyncGenerator[str, None]:
        #
        #     """Development Assistant - Professional development task support"""
        #     if not question:
        #         yield json.dumps({"error": "Question parameter cannot be empty"}, ensure_ascii=False)
        #         return
        #
        #     try:
        #         async for chunk in self.expert_service.ask_expert_stream(question):
        #             # Use base class standardized method to process chunk
        #             yield self._normalize_stream_chunk(chunk)
        #     except Exception as e:
        #         # Use base class error handling method
        #         yield await self._handle_stream_error("query_expert_stream", e)

        @self.streaming_tool(
            description="ğŸ¤– **Development Assistant** - Professional Development Task Executor\n\n" +
                        "## ğŸ› ï¸ Core Capabilities:\n" +
                        "â€¢ **Code Development** - Implementation in various programming languages\n" +
                        "â€¢ **Issue Diagnosis** - Bug fixes, performance optimization, error troubleshooting\n" +
                        "â€¢ **Architecture Design** - System design, technology selection, best practices\n" +
                        "â€¢ **File Operations** - Code refactoring, batch modifications, project building\n" +
                        "â€¢ **Environment Setup** - Development environment configuration, deployment setup, toolchain management\n\n" +
                        "## ğŸ“¤ Task Execution Results:\n" +
                        "â€¢ **Task Completed** - Successfully completed development task with complete solution\n" +
                        "â€¢ **Task Partially Completed** - Main functionality completed, with notes on incomplete parts and reasons\n" +
                        "â€¢ **Task Failed** - Unable to complete task, detailed failure reasons and suggestions provided\n" +
                        "â€¢ **Need More Information** - Task description insufficient, additional requirements needed\n" +
                        "â€¢ **Task Beyond Capability** - Task complexity exceeds current processing capability\n\n" +
                        "ğŸ’¡ **Working Method**: Automatically retrieves assigned development tasks, analyzes requirements, executes and provides execution status feedback."
        )
        async def query_expert_stream(
                question: Annotated[str, R(
                    "ğŸ¯ **Task Request Parameter**: Send task request to development assistant\n\n" +
                    "ğŸ“‹ **Standard Request Format**:\n" +
                    "â€¢ 'I have some development tasks, please help me complete them'\n" +
                    "â€¢ 'There are several development requirements to handle, please assist'\n" +
                    "â€¢ 'Please process the following development tasks'\n\n" +
                    "ğŸ“ **Manager Additional Instructions**:\n" +
                    "â€¢ 'Prioritize urgent tasks, focus on code quality'\n" +
                    "â€¢ 'Follow company coding standards, add detailed comments'\n" +
                    "â€¢ 'These tasks are complex, provide feedback if issues arise'\n" +
                    "â€¢ 'Use latest tech stack, ensure code maintainability'"
                )]
        ) -> AsyncGenerator[str, None]:
            """Development Assistant - Professional development task support"""
            if not question:
                yield json.dumps({"error": "Question parameter cannot be empty"}, ensure_ascii=False)
                return

            try:
                async for chunk in self.expert_service.ask_expert_stream(question):
                    # Use base class standardized method to process chunk
                    yield self._normalize_stream_chunk(chunk)
            except Exception as e:
                # Use base class error handling method
                yield await self._handle_stream_error("query_expert_stream", e)

        # @self.streaming_tool(
        #     description="ğŸ” **Task Inspector** - I am a professional task completion verification expert\n\n" +
        #                 "ğŸ‘‹ **Who I Am**: A professional inspector responsible for checking and verifying task execution status\n\n" +
        #                 "ğŸ¯ **My Responsibilities**:\n" +
        #                 "â€¢ Check if tasks are completed according to requirements\n" +
        #                 "â€¢ Verify code quality and best practices\n" +
        #                 "â€¢ Evaluate whether work results meet standards\n" +
        #                 "â€¢ Provide professional improvement suggestions\n\n" +
        #                 "ğŸ”§ **My Capabilities**:\n" +
        #                 "â€¢ Automatically retrieve current task details and requirements\n" +
        #                 "â€¢ Review all work submitted by executors\n" +
        #                 "â€¢ Conduct comprehensive quality assessment and code review\n" +
        #                 "â€¢ Provide clear completion status judgment (Completed/Incomplete)\n\n" +
        #                 "ğŸ’¬ **How to Communicate with Me**: Simply tell me what you want me to inspect, and I'll handle all technical details automatically"
        # )
        # async def query_expert_stream(
        #         question: Annotated[str, R(
        #             "ğŸ’¬ **Your Question**: Tell me what you want me to inspect\n\n" +
        #             "âœ… **Common Question Examples**:\n" +
        #             "â€¢ 'Please check if the current task has been completed'\n" +
        #             "â€¢ 'Help me verify the executor's work quality'\n" +
        #             "â€¢ 'Check the task completion status'\n" +
        #             "â€¢ 'Evaluate the current work results'"
        #         )]
        # ) -> AsyncGenerator[str, None]:
        #     """Development Assistant - Professional development task support"""
        #     if not question:
        #         yield json.dumps({"error": "Question parameter cannot be empty"}, ensure_ascii=False)
        #         return
        #
        #     try:
        #         async for chunk in self.expert_service.ask_expert_stream(question):
        #             # Use base class standardized method to process chunk
        #             yield self._normalize_stream_chunk(chunk)
        #     except Exception as e:
        #         # Use base class error handling method
        #         yield await self._handle_stream_error("query_expert_stream", e)

        @self.resource(uri="config://expert", name="Expert Config", description="ä¸“å®¶æœåŠ¡å™¨é…ç½®ä¿¡æ¯")
        async def expert_config_resource(uri: str) -> Dict[str, Any]:
            """è·å–ä¸“å®¶æœåŠ¡å™¨é…ç½®ä¿¡æ¯"""
            config = self.server_config
            mcp_servers_config = config.get("mcp_servers", "")
            mcp_servers = self._parse_mcp_servers_config(mcp_servers_config)

            return {
                "contents": [{
                    "uri": uri,
                    "mimeType": "application/json",
                    "text": json.dumps({
                        "config_info": {
                            "api_key": "***" if config.get("api_key") else "",
                            "base_url": config.get("base_url", "https://api.openai.com/v1"),
                            "model_name": config.get("model_name", "gpt-3.5-turbo"),
                            "system_prompt": config.get("system_prompt", ""),
                            "mcp_servers": mcp_servers
                        }
                    }, ensure_ascii=False)
                }]
            }
        
        return True

    @property
    def setup_server_params(self):
        """è®¾ç½®æœåŠ¡å™¨å‚æ•°è£…é¥°å™¨"""
        
        @self.decorators.server_param("api_key")
        async def api_key_param(
            param: Annotated[str, ServerParam(
                display_name="APIå¯†é’¥",
                description="OpenAI APIå¯†é’¥",
                param_type="password",
                required=True,
                placeholder="sk-..."
            )]
        ):
            """APIå¯†é’¥å‚æ•°"""
            pass
        
        @self.decorators.server_param("base_url")
        async def base_url_param(
            param: Annotated[str, StringParam(
                display_name="APIåŸºç¡€URL",
                description="APIåŸºç¡€URL",
                required=False,
                default_value="https://api.moonshot.cn/v1",
                placeholder="https://api.openai.com/v1"
            )]
        ):
            """APIåŸºç¡€URLå‚æ•°"""
            pass
        
        @self.decorators.server_param("model_name")
        async def model_name_param(
            param: Annotated[str, SelectParam(
                display_name="æ¨¡å‹åç§°",
                description="é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹",
                options=["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview", "gpt-4o", "kimi-k2-turbo-preview"],
                default_value="kimi-k2-turbo-preview",
                required=False
            )]
        ):
            """æ¨¡å‹åç§°å‚æ•°"""
            pass
        
        @self.decorators.server_param("system_prompt")
        async def system_prompt_param(
            param: Annotated[str, StringParam(
                display_name="ç³»ç»Ÿæç¤ºè¯",
                description="ç³»ç»Ÿæç¤ºè¯",
                required=False,
                default_value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿé€šè¿‡å·¥å…·å¸®ç”¨æŸ¥è¯¢å½“å‰ç›®å½•ä¸‹çš„å†…å®¹ã€‚",
                placeholder="è¾“å…¥ç³»ç»Ÿæç¤ºè¯..."
            )]
        ):
            """ç³»ç»Ÿæç¤ºè¯å‚æ•°"""
            pass
        
        @self.decorators.server_param("mcp_servers")
        async def mcp_servers_param(
            param: Annotated[str, StringParam(
                display_name="MCPæœåŠ¡å™¨é…ç½®",
                description="MCPæœåŠ¡å™¨é…ç½®ï¼ˆæ ¼å¼ï¼šname1:url1,name2:url2ï¼‰",
                required=False,
                default_value="file-reader: http://localhost:8001/mcp",
                placeholder="terminal-server:http://localhost:8001,file-server:http://localhost:8002"
            )]
        ):
            """MCPæœåŠ¡å™¨é…ç½®å‚æ•°"""
            pass
        
        @self.decorators.server_param("mongodb_url")
        async def mongodb_url_param(
            param: Annotated[str, StringParam(
                display_name="MongoDBè¿æ¥URL",
                description="MongoDBæ•°æ®åº“è¿æ¥URLï¼Œå¦‚æœä¸é…ç½®åˆ™ä½¿ç”¨æ–‡ä»¶å­˜å‚¨",
                required=False,
                default_value="",
                placeholder="mongodb://localhost:27017/chat_history"
            )]
        ):
            """MongoDBè¿æ¥URLå‚æ•°"""
            pass
        
        @self.decorators.server_param("history_limit")
        async def history_limit_param(
            param: Annotated[str, StringParam(
                display_name="å†å²è®°å½•æŸ¥è¯¢æ¡æ•°",
                description="æ¯æ¬¡å¯¹è¯æ—¶æŸ¥è¯¢çš„å†å²è®°å½•æ¡æ•°",
                required=False,
                default_value="10",
                placeholder="10"
            )]
        ):
            """å†å²è®°å½•æŸ¥è¯¢æ¡æ•°å‚æ•°"""
            pass
        
        @self.decorators.server_param("enable_history")
        async def enable_history_param(
            param: Annotated[bool, BooleanParam(
                display_name="å¯ç”¨èŠå¤©è®°å½•",
                description="æ˜¯å¦å¯ç”¨èŠå¤©è®°å½•åŠŸèƒ½",
                required=False,
                default_value=False,
            )]
        ):
            """å¯ç”¨èŠå¤©è®°å½•å‚æ•°"""
            pass
        
        @self.decorators.server_param("role")
        async def role_param(
            param: Annotated[str, StringParam(
                display_name="è§’è‰²è®¾å®š",
                description="AIåŠ©æ‰‹çš„è§’è‰²è®¾å®š",
                required=False,
                default_value="",
                placeholder="è¾“å…¥è§’è‰²è®¾å®š..."
            )]
        ):
            """è§’è‰²è®¾å®šå‚æ•°"""
            pass
        
        return True

    async def initialize(self) -> None:
        """åˆå§‹åŒ–æœåŠ¡å™¨"""
        # ä½¿ç”¨æ¡†æ¶çš„é€šç”¨é…ç½®å¤„ç†æµç¨‹
        config_values = self._setup_decorators_and_log_config(
            required_keys=["api_key"],
            config_defaults={
                "api_key": None,
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-3.5-turbo",
                "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæä¾›å‡†ç¡®ã€è¯¦ç»†å’Œæœ‰ç”¨çš„å›ç­”ã€‚",
                "mcp_servers": ""
            }
        )

        # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸“å®¶æœåŠ¡
        self.expert_service = await ExpertService.from_config(config_values)

        # ä½¿ç”¨æ¡†æ¶çš„å·¥å…·æ—¥å¿—è®°å½•åŠŸèƒ½
        self._log_tools_info()

        logger.info("âœ… Expert MCP Server (Annotated) initialized successfully")

    async def shutdown(self) -> None:
        """å…³é—­æœåŠ¡å™¨"""
        if self._initialized and self.expert_service:
            # æ­£ç¡®å…³é—­ä¸“å®¶æœåŠ¡
            try:
                await self.expert_service.shutdown()
            except Exception as e:
                logger.warning(f"å…³é—­ä¸“å®¶æœåŠ¡æ—¶å‡ºç°è­¦å‘Š: {e}")
            finally:
                self.expert_service = None
                logger.info("ğŸ›‘ Expert Serviceå·²æ¸…ç†")
        
        # è°ƒç”¨çˆ¶ç±»çš„shutdownæ–¹æ³•
        await super().shutdown()


