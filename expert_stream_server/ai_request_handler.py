import logging
from typing import Any, Dict, List, AsyncGenerator
from openai import AsyncOpenAI

# é…ç½®æ—¥å¿—
logger = logging.getLogger("AiRequestHandler")


class AiRequestHandler:
    """AIè¯·æ±‚å¤„ç†å™¨"""

    def __init__(self, messages: List[Dict[str, Any]], tools: List[Dict[str, Any]],
                 conversation_id: str, callback, model_config: Dict[str, Any]):
        self.messages = messages
        self.tools = tools
        self.conversation_id = conversation_id
        self.callback = callback
        self.model_config = model_config
        self.is_aborted = False
        self.client = None

    async def _get_client(self):
        """è·å–æˆ–åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
        if self.client is None:
            self.client = AsyncOpenAI(
                api_key=self.model_config['api_key'],
                base_url=self.model_config['base_url']
            )
        return self.client

    async def _close_client(self):
        """å®‰å…¨å…³é—­å®¢æˆ·ç«¯"""
        if self.client is not None:
            try:
                # ç¡®ä¿æ‰€æœ‰å¼‚æ­¥ç”Ÿæˆå™¨éƒ½è¢«æ­£ç¡®å…³é—­
                await self.client.close()
                logger.debug("OpenAIå®¢æˆ·ç«¯å·²æˆåŠŸå…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­OpenAIå®¢æˆ·ç«¯æ—¶å‡ºç°è­¦å‘Š: {e}")
            finally:
                self.client = None

    # chat_completion éæµå¼æ–¹æ³•å·²ç§»é™¤

    async def chat_completion_stream(self) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œæµå¼èŠå¤©å®Œæˆ"""
        stream = None
        try:
            if self.callback:
                self.callback('ai_request_start', {
                    'conversation_id': self.conversation_id,
                    'messages_count': len(self.messages)
                })

            # è·å–å®¢æˆ·ç«¯
            client = await self._get_client()
            
            payload = self.build_payload()
            payload['stream'] = True

            stream = await client.chat.completions.create(**payload)

            # åˆå§‹åŒ–ç´¯ç§¯å˜é‡
            content = ""
            tool_calls = []  # ä½¿ç”¨åˆ—è¡¨æ¥æŒ‰ç´¢å¼•å­˜å‚¨

            try:
                async for chunk in stream:
                    if self.is_aborted:
                        logger.info("æµå¼å¤„ç†è¢«ä¸­æ­¢")
                        break

                    if chunk.choices and chunk.choices[0].delta:
                        delta = chunk.choices[0].delta

                        # å¤„ç†å†…å®¹
                        if delta.content:
                            content += delta.content
                            if self.callback:
                                self.callback('ai_stream_chunk', {
                                    'content': delta.content
                                })
                            # è¿”å›JSONæ ¼å¼çš„å¢é‡å†…å®¹ï¼Œè€Œä¸æ˜¯ç´¯ç§¯å†…å®¹
                            import json
                            yield json.dumps({"type": "content", "data": delta.content}, ensure_ascii=False)

                        # å¤„ç†å·¥å…·è°ƒç”¨
                        if delta.tool_calls:
                            for tool_call in delta.tool_calls:
                                self._process_tool_call_delta(tool_calls, tool_call)

                        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                        if chunk.choices[0].finish_reason:
                            break
            except Exception as stream_error:
                logger.error(f"æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {stream_error}")
                # ç¡®ä¿æµè¢«æ­£ç¡®å…³é—­
                if stream and hasattr(stream, 'close'):
                    try:
                        await stream.close()
                    except Exception as close_error:
                        logger.warning(f"å…³é—­æµæ—¶å‡ºç°è­¦å‘Š: {close_error}")
                raise stream_error

            # æ„å»ºæœ€ç»ˆç»“æœ
            result = {
                'role': 'assistant',
                'content': content,
                'tool_calls': [call for call in tool_calls if call.get('id')]  # è¿‡æ»¤æ‰ç©ºçš„tool_calls
            }

            # åªæœ‰å½“æœ‰å†…å®¹æˆ–å·¥å…·è°ƒç”¨æ—¶æ‰æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            if result['content'] or result['tool_calls']:
                self.messages.append(result)

            if self.callback:
                self.callback('ai_response', {
                    'content': result['content'],
                    'tool_calls': result['tool_calls'],
                    'conversation_id': self.conversation_id
                })

        except Exception as e:
            logger.error(f"Stream chat completion failed: {e}")
            if self.callback:
                self.callback('error', {
                    'message': str(e),
                    'conversation_id': self.conversation_id
                })
            raise e
        finally:
            # ç¡®ä¿æµè¢«æ­£ç¡®å…³é—­
            if stream and hasattr(stream, 'close'):
                try:
                    await stream.close()
                except Exception as close_error:
                    logger.warning(f"åœ¨finallyä¸­å…³é—­æµæ—¶å‡ºç°è­¦å‘Š: {close_error}")
            # ç¡®ä¿å®¢æˆ·ç«¯è¢«æ­£ç¡®å…³é—­
            await self._close_client()

    # handle_stream_response éæµå¼æ–¹æ³•å·²ç§»é™¤

    def build_payload(self) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚è½½è·"""
        payload = {
            'model': self.model_config['model_name'],
            'messages': self.messages,
            'temperature': self.model_config.get('temperature', 0.7),
            'max_tokens': 16000
        }

        if self.tools:
            payload['tools'] = self.tools
            payload['tool_choice'] = 'auto'
            logger.info(
                f"ğŸ¤– AIè¯·æ±‚å‚æ•°: æ¨¡å‹={payload['model']}, æ¶ˆæ¯æ•°={len(payload['messages'])}, å·¥å…·æ•°={len(payload['tools'])}, æ¸©åº¦={payload['temperature']}, æœ€å¤§ä»¤ç‰Œ={payload['max_tokens']}")
        else:
            logger.info(
                f"ğŸ¤– AIè¯·æ±‚å‚æ•°: æ¨¡å‹={payload['model']}, æ¶ˆæ¯æ•°={len(payload['messages'])}, æ— å·¥å…·, æ¸©åº¦={payload['temperature']}, æœ€å¤§ä»¤ç‰Œ={payload['max_tokens']}")

        return payload

    def abort(self):
        """ä¸­æ­¢è¯·æ±‚"""
        self.is_aborted = True
        # ç«‹å³å…³é—­å®¢æˆ·ç«¯è¿æ¥ä»¥é¿å…å¼‚æ­¥ç”Ÿæˆå™¨é”™è¯¯
        if self.client is not None:
            import asyncio
            try:
                # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æ¥å¼‚æ­¥å…³é—­å®¢æˆ·ç«¯
                asyncio.create_task(self._close_client())
            except Exception as e:
                logger.warning(f"åœ¨abortä¸­å…³é—­å®¢æˆ·ç«¯æ—¶å‡ºç°è­¦å‘Š: {e}")

    def _process_tool_call_delta(self, tool_calls_list: List, tool_call_delta):
        """ç»Ÿä¸€å¤„ç†å·¥å…·è°ƒç”¨å¢é‡æ•°æ®"""
        if not hasattr(tool_call_delta, 'index') or tool_call_delta.index is None:
            logger.warning("Tool call delta missing index, skipping")
            return

        index = tool_call_delta.index

        # ç¡®ä¿åˆ—è¡¨æœ‰è¶³å¤Ÿçš„å…ƒç´ 
        while len(tool_calls_list) <= index:
            tool_calls_list.append({
                'id': '',
                'type': 'function',
                'function': {'name': '', 'arguments': ''}
            })

        current_call = tool_calls_list[index]

        # åªåœ¨æœ‰å€¼æ—¶æ›´æ–°ï¼ˆé¿å…è¦†ç›–å·²æœ‰å€¼ï¼‰
        if hasattr(tool_call_delta, 'id') and tool_call_delta.id:
            current_call['id'] = tool_call_delta.id
        if hasattr(tool_call_delta, 'type') and tool_call_delta.type:
            current_call['type'] = tool_call_delta.type

        if hasattr(tool_call_delta, 'function') and tool_call_delta.function:
            if hasattr(tool_call_delta.function, 'name') and tool_call_delta.function.name:
                current_call['function']['name'] = tool_call_delta.function.name
            if hasattr(tool_call_delta.function, 'arguments') and tool_call_delta.function.arguments:
                current_call['function']['arguments'] += tool_call_delta.function.arguments


