import json
import logging
import uuid
from typing import Dict, List, AsyncGenerator

# ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼Œä½†ä¸åŒ…å«åŒ…åå‰ç¼€
from ai_client import AiClient
from ai_request_handler import AiRequestHandler
from chat_history_manager import ChatHistoryManager
from mcp_tool_execute import McpToolExecute
from mcp_framework.core import parse_mcp_servers_config

# é…ç½®æ—¥å¿—
logger = logging.getLogger("ExpertService")


def parse_stdio_mcp_servers_config(config_str: str) -> List[Dict[str, str]]:
    """è§£æ stdio MCP æœåŠ¡å™¨é…ç½®å­—ç¬¦ä¸²
    
    æ ¼å¼: name1:script_path1--alias,name2:script_path2--alias
    ä¾‹å¦‚: file-manager:file_manager.py--file-mgr,task-runner:task_runner.js--task-mgr
    """
    if not config_str or not config_str.strip():
        return []
    
    servers = []
    for server_config in config_str.split(','):
        server_config = server_config.strip()
        if not server_config:
            continue
            
        try:
            # åˆ†å‰² name:script_path--alias
            if '--' in server_config:
                name_script, alias = server_config.split('--', 1)
                name, script_path = name_script.split(':', 1)
                
                servers.append({
                    'name': name.strip(),
                    'command': script_path.strip(),
                    'alias': alias.strip()
                })
            else:
                # å…¼å®¹æ²¡æœ‰ alias çš„æ ¼å¼
                name, script_path = server_config.split(':', 1)
                servers.append({
                    'name': name.strip(),
                    'command': script_path.strip(),
                    'alias': name.strip()  # ä½¿ç”¨ name ä½œä¸º alias
                })
        except ValueError as e:
            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„stdioæœåŠ¡å™¨é…ç½®: {server_config} - {e}")
            continue
    
    return servers


class ExpertService:
    """ä¸“å®¶æœåŠ¡ç±»"""

    @classmethod
    async def from_config(cls, config_values: Dict[str, any]) -> 'ExpertService':
        """ä»é…ç½®å­—å…¸åˆ›å»ºå¹¶åˆå§‹åŒ– ExpertService å®ä¾‹"""
        # è§£æMCPæœåŠ¡å™¨é…ç½®
        mcp_servers = parse_mcp_servers_config(config_values["mcp_servers"])
        if mcp_servers:
            logger.info(f"ğŸ”§ è§£æåˆ°çš„MCPæœåŠ¡å™¨: {mcp_servers}")
        
        # è§£æ stdio MCP æœåŠ¡å™¨é…ç½®
        stdio_mcp_servers = parse_stdio_mcp_servers_config(config_values.get("stdio_mcp_servers", ""))
        if stdio_mcp_servers:
            logger.info(f"ğŸ”§ è§£æåˆ°çš„Stdio MCPæœåŠ¡å™¨: {stdio_mcp_servers}")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        service = cls(
            api_key=config_values["api_key"],
            base_url=config_values["base_url"],
            model_name=config_values["model_name"],
            system_prompt=config_values["system_prompt"],
            mcp_servers=mcp_servers,
            stdio_mcp_servers=stdio_mcp_servers,
            mongodb_url=config_values.get("mongodb_url", ""),
            history_limit=config_values.get("history_limit", 10),
            enable_history=config_values.get("enable_history", True),
            role=config_values.get("role", ""),
            summary_interval=config_values.get("summary_interval", 5),
            max_rounds=config_values.get("max_rounds", 25),
            summary_instruction=config_values.get("summary_instruction", ""),
            summary_request=config_values.get("summary_request", ""),
            summary_length_threshold=config_values.get("summary_length_threshold", 30000)
        )
        
        # åˆå§‹åŒ–æœåŠ¡
        await service.initialize()
        
        return service

    def __init__(self, api_key: str, base_url: str = "https://api.openai.com/v1",
                 model_name: str = "gpt-3.5-turbo", system_prompt: str = "",
                 mcp_servers: List[Dict[str, str]] = None, 
                 stdio_mcp_servers: List[Dict[str, str]] = None,
                 mongodb_url: str = "",
                 history_limit: int = 10, enable_history: bool = True, role: str = "",
                 summary_interval: int = 5, max_rounds: int = 25,
                 summary_instruction: str = "", summary_request: str = "",
                 summary_length_threshold: int = 30000):
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name
        self.system_prompt = system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæä¾›å‡†ç¡®ã€è¯¦ç»†å’Œæœ‰ç”¨çš„å›ç­”ã€‚"
        self.mcp_servers = mcp_servers or []
        self.stdio_mcp_servers = stdio_mcp_servers or []
        self.role = role

        # èŠå¤©å†å²é…ç½®
        self.enable_history = enable_history
        self.history_limit = history_limit
        
        # å·¥å…·è°ƒç”¨æ€»ç»“é…ç½®
        self.summary_interval = summary_interval
        self.max_rounds = max_rounds
        self.summary_instruction = summary_instruction
        self.summary_length_threshold = summary_length_threshold
        self.summary_request = summary_request

        # ç”Ÿæˆå›ºå®šçš„ä¼šè¯IDï¼Œæ•´ä¸ªæœåŠ¡ç”Ÿå‘½å‘¨æœŸå†…ä½¿ç”¨åŒä¸€ä¸ª
        self.conversation_id = f"expert_conv_{uuid.uuid4().hex[:16]}"

        # ç§»é™¤åœæ­¢æ ‡å¿—ï¼Œä½¿ç”¨æ¡†æ¶æä¾›çš„åœæ­¢åŠŸèƒ½

        # åˆå§‹åŒ–MCPå·¥å…·æ‰§è¡Œå™¨
        self.mcp_tool_execute = McpToolExecute(self.mcp_servers, self.stdio_mcp_servers, role=self.role)

        # åˆå§‹åŒ–èŠå¤©è®°å½•ç®¡ç†å™¨
        self.chat_history = ChatHistoryManager(
            mongodb_url=mongodb_url,
            history_limit=history_limit,
            enable_history=enable_history
        )

        logger.info(f"Expert Service initialized with model: {self.model_name}")
        logger.info(f"Configured MCP servers: {len(self.mcp_servers)}")
        logger.info(f"Configured Stdio MCP servers: {len(self.stdio_mcp_servers)}")
        logger.info(f"Chat history enabled: {enable_history}, limit: {history_limit}")
        logger.info(f"Summary interval: {summary_interval} rounds")
        logger.info(f"Max rounds: {max_rounds} rounds")
        logger.info(f"Fixed conversation ID: {self.conversation_id}")

    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # æ£€æŸ¥æ˜¯å¦åœ¨æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡ MCP å·¥å…·åˆå§‹åŒ–
        import os
        has_mcp_servers = bool(self.mcp_servers or self.stdio_mcp_servers)
        if os.getenv("TESTING_MODE") == "true" or not has_mcp_servers:
            logger.info("ğŸ§ª æµ‹è¯•ç¯å¢ƒæˆ–æ— MCPæœåŠ¡å™¨é…ç½®ï¼Œè·³è¿‡MCPå·¥å…·åˆå§‹åŒ–")
            # åœ¨æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œåˆå§‹åŒ–ç©ºçš„å·¥å…·åˆ—è¡¨
            self.mcp_tool_execute.tools = []
            self.mcp_tool_execute.tool_metadata = {}
        else:
            await self.mcp_tool_execute.init()
        await self.chat_history.initialize()

    async def shutdown(self):
        """å…³é—­æœåŠ¡ï¼Œæ¸…ç†èµ„æº"""
        logger.info("ğŸ›‘ æ­£åœ¨å…³é—­ExpertæœåŠ¡...")
        
        # æ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†åœæ­¢é€»è¾‘
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿abortæ“ä½œå®Œæˆ
        import asyncio
        await asyncio.sleep(0.1)
        
        # æ¸…ç†MCPå·¥å…·æ‰§è¡Œå™¨
        if hasattr(self.mcp_tool_execute, 'cleanup'):
            try:
                await self.mcp_tool_execute.cleanup()
            except Exception as e:
                logger.warning(f"æ¸…ç†MCPå·¥å…·æ‰§è¡Œå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # æ¸…ç†èŠå¤©å†å²ç®¡ç†å™¨
        if hasattr(self.chat_history, 'close'):
            try:
                await self.chat_history.close()
            except Exception as e:
                logger.warning(f"å…³é—­èŠå¤©å†å²ç®¡ç†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        logger.info("âœ… ExpertæœåŠ¡å·²å…³é—­")

    async def _summarize_history(self, history_messages: List[Dict]) -> str:
        """ä½¿ç”¨AIæ€»ç»“å†å²è®°å½•å†…å®¹"""
        if not history_messages:
            return ""

        # æ„å»ºå†å²è®°å½•æ–‡æœ¬
        history_text = ""
        for msg in history_messages:
            role = msg['role']
            content = msg['content']
            metadata = msg.get('metadata', {})
            msg_type = metadata.get('type', 'normal')
            
            if role == 'user':
                role_name = "ç”¨æˆ·"
            elif role == 'assistant':
                if msg_type == 'tool_call':
                    role_name = "åŠ©æ‰‹(å·¥å…·è°ƒç”¨)"
                else:
                    role_name = "åŠ©æ‰‹"
            elif role == 'tool':
                role_name = "å·¥å…·æ‰§è¡Œç»“æœ"
            else:
                role_name = role
                
            history_text += f"{role_name}: {content}\n"

        # æ„å»ºæ€»ç»“è¯·æ±‚
        summary_messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²çš„ä¸»è¦å†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œé‡ç‚¹å…³æ³¨ç”¨æˆ·çš„éœ€æ±‚å’Œå·²è®¨è®ºçš„å…³é”®ä¿¡æ¯ã€‚æ€»ç»“åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—ã€‚"
            },
            {
                "role": "user",
                "content": f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²ï¼š\n\n{history_text}"
            }
        ]

        try:
            # åˆ›å»ºAIå®¢æˆ·ç«¯è¿›è¡Œæ€»ç»“
            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.3,
                'max_tokens': 16000
            }

            ai_handler = AiRequestHandler(
                messages=summary_messages,
                tools=[],  # æ€»ç»“æ—¶ä¸éœ€è¦å·¥å…·
                conversation_id=f"summary_{uuid.uuid4().hex[:8]}",
                callback=None,
                model_config=model_config
            )

            # ä½¿ç”¨æµå¼æ–¹æ³•æ›¿ä»£å·²ç§»é™¤çš„éæµå¼æ–¹æ³•
            content_chunks = []
            
            # ä½¿ç”¨æµå¼æ–¹æ³•è·å–å“åº”
            async for chunk in ai_handler.chat_completion_stream():
                import json
                try:
                    chunk_data = json.loads(chunk)
                    if chunk_data.get('type') == 'content' and chunk_data.get('data'):
                        content_chunks.append(chunk_data.get('data'))
                except Exception as e:
                    logger.error(f"å¤„ç†æ€»ç»“æµå¼å“åº”æ—¶å‡ºé”™: {e}")
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹å—ä½œä¸ºæ€»ç»“
            summary = "".join(content_chunks) if content_chunks else ""

            logger.info(f"ğŸ“ å†å²è®°å½•æ€»ç»“å®Œæˆ: {summary[:50]}...")
            return summary

        except Exception as e:
            logger.error(f"âŒ å†å²è®°å½•æ€»ç»“å¤±è´¥: {e}")
            return ""

    async def ask_expert(self, question: str) -> str:
        """å‘AIä¸“å®¶æé—®"""
        try:
            logger.info(f"ğŸ¤– æ”¶åˆ°é—®é¢˜: {question[:100]}..." if len(question) > 100 else f"ğŸ¤– æ”¶åˆ°é—®é¢˜: {question}")

            # ä½¿ç”¨å›ºå®šçš„ä¼šè¯ID
            conversation_id = self.conversation_id

            # æ„å»ºsystem prompt
            system_prompt = self.system_prompt

            # å¦‚æœå¯ç”¨èŠå¤©å†å²ï¼Œè·å–å¹¶æ€»ç»“å†å²è®°å½•
            if self.enable_history:
                history_messages = await self.chat_history.get_history(conversation_id, self.history_limit)
                logger.info(f"ğŸ“ åŠ è½½å†å²è®°å½•: {len(history_messages)} æ¡")

                if history_messages:
                    # æ€»ç»“å†å²è®°å½•
                    history_summary = await self._summarize_history(history_messages)
                    if history_summary:
                        system_prompt += f"\n\nã€å¯¹è¯å†å²æ€»ç»“ã€‘\n{history_summary}"
                        logger.info(f"ğŸ“ å·²å°†å†å²è®°å½•æ€»ç»“åŠ å…¥system prompt")

                # ä¿å­˜ç”¨æˆ·é—®é¢˜
                await self.chat_history.save_message(conversation_id, "user", question)

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåªåŒ…å«system promptå’Œå½“å‰é—®é¢˜ï¼‰
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]

            logger.info(f"ğŸ¤– æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")

            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.7,
                'max_tokens': 2000
            }

            # è·å–å¯ç”¨å·¥å…·
            tools = self.mcp_tool_execute.get_tools()
            logger.info(f"ğŸ› ï¸ è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")

            logger.info(f"ğŸ¤– åˆ›å»ºAIå®¢æˆ·ç«¯ï¼Œä¼šè¯ID: {conversation_id}")

            ai_client = AiClient(
                messages, conversation_id, tools, model_config,
                None, self.mcp_tool_execute,
                summary_interval=self.summary_interval,
                max_rounds=self.max_rounds,
                summary_instruction=self.summary_instruction,
                summary_request=self.summary_request,
                summary_length_threshold=self.summary_length_threshold
            )

            # å¼€å§‹å¤„ç†
            logger.info(f"ğŸ¤– å¼€å§‹AIå¯¹è¯å¤„ç†")
            await ai_client.start()

            # è¿”å›æœ€åçš„åŠ©æ‰‹å›å¤
            for message in reversed(messages):
                if message.get('role') == 'assistant' and message.get('content'):
                    result = message['content']
                    # åªåœ¨å¯ç”¨å†å²è®°å½•æ—¶ä¿å­˜åŠ©æ‰‹å›å¤
                    if self.enable_history:
                        await self.chat_history.save_message(conversation_id, "assistant", result)
                        logger.info(f"ğŸ’¾ å·²ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²ï¼Œä¼šè¯ID: {conversation_id}")
                    logger.info(f"ğŸ¤– AIå›ç­”: {result[:200]}..." if len(result) > 200 else f"ğŸ¤– AIå›ç­”: {result}")
                    return result

            logger.warning(f"âš ï¸ æ²¡æœ‰è·å¾—æœ‰æ•ˆçš„AIå›å¤")
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰è·å¾—æœ‰æ•ˆå›å¤ã€‚"

        except Exception as e:
            logger.error(f"âŒ Ask expert failed: {e}")
            return f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"

    async def ask_expert_stream(self, question: str) -> AsyncGenerator[str, None]:
        """å‘AIä¸“å®¶æé—®ï¼ˆæµå¼å“åº”ï¼‰"""
        try:
            logger.info(
                f"ğŸŒŠ æ”¶åˆ°æµå¼é—®é¢˜: {question[:100]}..." if len(question) > 100 else f"ğŸŒŠ æ”¶åˆ°æµå¼é—®é¢˜: {question}")

            # ä½¿ç”¨å›ºå®šçš„ä¼šè¯ID
            conversation_id = self.conversation_id

            # æ„å»ºsystem prompt
            system_prompt = self.system_prompt

            # å¦‚æœå¯ç”¨èŠå¤©å†å²ï¼Œè·å–å¹¶æ€»ç»“å†å²è®°å½•
            if self.enable_history:
                history_messages = await self.chat_history.get_history(conversation_id, self.history_limit)
                logger.info(f"ğŸ“ æµå¼æ¨¡å¼åŠ è½½å†å²è®°å½•: {len(history_messages)} æ¡")

                if history_messages:
                    # æ€»ç»“å†å²è®°å½•
                    history_summary = await self._summarize_history(history_messages)
                    if history_summary:
                        system_prompt += f"\n\nã€å¯¹è¯å†å²æ€»ç»“ã€‘\n{history_summary}"
                        logger.info(f"ğŸ“ å·²å°†å†å²è®°å½•æ€»ç»“åŠ å…¥æµå¼system prompt")

                # ä¿å­˜ç”¨æˆ·é—®é¢˜
                await self.chat_history.save_message(conversation_id, "user", question)

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåªåŒ…å«system promptå’Œå½“å‰é—®é¢˜ï¼‰
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]

            logger.info(f"ğŸŒŠ æ„å»ºæµå¼æ¶ˆæ¯åˆ—è¡¨ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")

            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.7,
                'max_tokens': 2000
            }

            # è·å–å¯ç”¨å·¥å…·
            tools = self.mcp_tool_execute.get_tools()
            logger.info(f"ğŸ› ï¸ æµå¼æ¨¡å¼è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")

            # åˆ›å»ºæµå¼å›è°ƒ
            stream_data = []
            assistant_response = ""
            pending_newlines = []  # å­˜å‚¨å¾…å‘é€çš„å›è½¦ç¬¦å·

            def stream_callback(event_type, data):
                logger.info(f"ğŸŒŠ æµå¼å›è°ƒ: {event_type} - {str(data)[:100]}..." if len(
                    str(data)) > 100 else f"ğŸŒŠ æµå¼å›è°ƒ: {event_type} - {data}")
                stream_data.append({"type": event_type, "data": data})
                
                # ä¿å­˜å·¥å…·è°ƒç”¨å’Œå·¥å…·ç»“æœåˆ°å†å²è®°å½•
                if self.enable_history:
                    if event_type == 'tool_call' and isinstance(data, list):
                        # åœ¨å·¥å…·è°ƒç”¨å‰æ·»åŠ å›è½¦ç¬¦å·
                        pending_newlines.append("before_tool_call")
                        # ä¿å­˜å·¥å…·è°ƒç”¨ä¿¡æ¯
                        for tool_call in data:
                            if isinstance(tool_call, dict):
                                tool_call_content = f"è°ƒç”¨å·¥å…·: {tool_call.get('function', {}).get('name', 'unknown')}\nå‚æ•°: {tool_call.get('function', {}).get('arguments', '')}"
                                import asyncio
                                asyncio.create_task(self.chat_history.save_message(
                                    conversation_id, "assistant", tool_call_content, 
                                    {"type": "tool_call", "tool_call_id": tool_call.get('id')}
                                ))
                    elif event_type == 'tool_result' and isinstance(data, list):
                        # ä¿å­˜å·¥å…·æ‰§è¡Œç»“æœ
                        for tool_result in data:
                            if isinstance(tool_result, dict):
                                tool_result_content = f"å·¥å…· {tool_result.get('name', 'unknown')} æ‰§è¡Œç»“æœ:\n{tool_result.get('content', '')}"
                                import asyncio
                                asyncio.create_task(self.chat_history.save_message(
                                    conversation_id, "tool", tool_result_content,
                                    {"type": "tool_result", "tool_call_id": tool_result.get('tool_call_id')}
                                ))
                        # åœ¨å·¥å…·ç»“æœåæ·»åŠ å›è½¦ç¬¦å·
                        pending_newlines.append("after_tool_result")

            logger.info(f"ğŸŒŠ åˆ›å»ºæµå¼AIå®¢æˆ·ç«¯ï¼Œä¼šè¯ID: {conversation_id}")

            ai_client = AiClient(
                messages, conversation_id, tools, model_config,
                stream_callback, self.mcp_tool_execute,
                summary_interval=self.summary_interval,
                max_rounds=self.max_rounds,
                summary_instruction=self.summary_instruction,
                summary_request=self.summary_request,
                summary_length_threshold=self.summary_length_threshold
            )

            # AIå®¢æˆ·ç«¯ç°åœ¨ç”±æ¡†æ¶ç®¡ç†

            # å¼€å§‹æµå¼å¯¹è¯
            logger.info(f"ğŸŒŠ å¼€å§‹æµå¼AIå¯¹è¯å¤„ç†")
            chunk_count = 0
            assistant_content = ""
            last_pending_count = 0

            async for chunk in ai_client.start_stream():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å‘é€çš„å›è½¦ç¬¦å·
                if len(pending_newlines) > last_pending_count:
                    # æœ‰æ–°çš„å›è½¦ç¬¦å·éœ€è¦å‘é€
                    for i in range(last_pending_count, len(pending_newlines)):
                        newline_type = pending_newlines[i]
                        logger.info(f"ğŸŒŠ å‘é€å›è½¦ç¬¦å·: {newline_type}")
                        yield json.dumps({"type": "content", "data": "\n"}, ensure_ascii=False)
                    last_pending_count = len(pending_newlines)

                chunk_count += 1
                logger.info(f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk[:50]}..." if len(
                    chunk) > 50 else f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk}")

                # æ”¶é›†åŠ©æ‰‹å›å¤å†…å®¹ç”¨äºä¿å­˜åˆ°å†å²è®°å½•
                try:
                    chunk_data = json.loads(chunk)
                    # ç¡®ä¿chunk_dataæ˜¯å­—å…¸ç±»å‹æ‰è°ƒç”¨.get()æ–¹æ³•
                    if isinstance(chunk_data, dict) and chunk_data.get("type") == "content" and "data" in chunk_data:
                        assistant_content += chunk_data["data"]
                except (json.JSONDecodeError, KeyError, AttributeError):
                    pass

                yield chunk

            # å¤„ç†æœ€åå¯èƒ½å‰©ä½™çš„å›è½¦ç¬¦å·
            if len(pending_newlines) > last_pending_count:
                for i in range(last_pending_count, len(pending_newlines)):
                    newline_type = pending_newlines[i]
                    logger.info(f"ğŸŒŠ å‘é€æœ€åçš„å›è½¦ç¬¦å·: {newline_type}")
                    yield json.dumps({"type": "content", "data": "\n"}, ensure_ascii=False)

            # ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²
            if self.enable_history and assistant_content.strip():
                await self.chat_history.save_message(
                    conversation_id, "assistant", assistant_content.strip()
                )
                logger.info(f"ğŸ’¾ å·²ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²ï¼Œä¼šè¯ID: {conversation_id}")

            logger.info(f"ğŸŒŠ æµå¼å¯¹è¯å®Œæˆï¼Œæ€»å…±äº§ç”Ÿ {chunk_count} ä¸ªå—")

        except Exception as e:
            logger.error(f"âŒ Ask expert stream failed: {e}")
            yield json.dumps({"type": "error", "data": f"æŸ¥è¯¢ä¸“å®¶å¤±è´¥: {str(e)}"}, ensure_ascii=False)
