import json
import logging
import uuid
from typing import Dict, List, AsyncGenerator

# ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼Œä½†ä¸åŒ…å«åŒ…åå‰ç¼€
from ai_client import AiClient
from ai_request_handler import AiRequestHandler
from chat_history_manager import ChatHistoryManager
from mcp_tool_execute import McpToolExecute
from mcp_framework.core import parse_mcp_servers_config

# é…ç½®æ—¥å¿—
logger = logging.getLogger("ExpertService")


class ExpertService:
    """ä¸“å®¶æœåŠ¡ç±»"""

    @classmethod
    async def from_config(cls, config_values: Dict[str, any]) -> 'ExpertService':
        """ä»é…ç½®å­—å…¸åˆ›å»ºå¹¶åˆå§‹åŒ– ExpertService å®ä¾‹"""
        # è§£æMCPæœåŠ¡å™¨é…ç½®
        mcp_servers = parse_mcp_servers_config(config_values["mcp_servers"])
        if mcp_servers:
            logger.info(f"ğŸ”§ è§£æåˆ°çš„MCPæœåŠ¡å™¨: {mcp_servers}")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        service = cls(
            api_key=config_values["api_key"],
            base_url=config_values["base_url"],
            model_name=config_values["model_name"],
            system_prompt=config_values["system_prompt"],
            mcp_servers=mcp_servers
        )
        
        # åˆå§‹åŒ–æœåŠ¡
        await service.initialize()
        
        return service

    def __init__(self, api_key: str, base_url: str = "https://api.openai.com/v1",
                 model_name: str = "gpt-3.5-turbo", system_prompt: str = "",
                 mcp_servers: List[Dict[str, str]] = None, mongodb_url: str = "",
                 history_limit: int = 10, enable_history: bool = True):
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name
        self.system_prompt = system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæä¾›å‡†ç¡®ã€è¯¦ç»†å’Œæœ‰ç”¨çš„å›ç­”ã€‚"
        self.mcp_servers = mcp_servers or []

        # èŠå¤©å†å²é…ç½®
        self.enable_history = enable_history
        self.history_limit = history_limit

        # ç”Ÿæˆå›ºå®šçš„ä¼šè¯IDï¼Œæ•´ä¸ªæœåŠ¡ç”Ÿå‘½å‘¨æœŸå†…ä½¿ç”¨åŒä¸€ä¸ª
        self.conversation_id = f"expert_conv_{uuid.uuid4().hex[:16]}"

        # ç§»é™¤åœæ­¢æ ‡å¿—ï¼Œä½¿ç”¨æ¡†æ¶æä¾›çš„åœæ­¢åŠŸèƒ½

        # åˆå§‹åŒ–MCPå·¥å…·æ‰§è¡Œå™¨
        self.mcp_tool_execute = McpToolExecute(self.mcp_servers)

        # åˆå§‹åŒ–èŠå¤©è®°å½•ç®¡ç†å™¨
        self.chat_history = ChatHistoryManager(
            mongodb_url=mongodb_url,
            history_limit=history_limit,
            enable_history=enable_history
        )

        logger.info(f"Expert Service initialized with model: {self.model_name}")
        logger.info(f"Configured MCP servers: {len(self.mcp_servers)}")
        logger.info(f"Chat history enabled: {enable_history}, limit: {history_limit}")
        logger.info(f"Fixed conversation ID: {self.conversation_id}")

    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        await self.mcp_tool_execute.init()
        await self.chat_history.initialize()

    async def shutdown(self):
        """å…³é—­æœåŠ¡ï¼Œæ¸…ç†èµ„æº"""
        logger.info("ğŸ›‘ æ­£åœ¨å…³é—­ExpertæœåŠ¡...")
        
        # æ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†åœæ­¢é€»è¾‘
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿abortæ“ä½œå®Œæˆ
        import asyncio
        await asyncio.sleep(0.1)
        
        # æ¸…ç†MCPå·¥å…·æ‰§è¡Œå™¨
        if hasattr(self.mcp_tool_execute, 'cleanup'):
            try:
                await self.mcp_tool_execute.cleanup()
            except Exception as e:
                logger.warning(f"æ¸…ç†MCPå·¥å…·æ‰§è¡Œå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # æ¸…ç†èŠå¤©å†å²ç®¡ç†å™¨
        if hasattr(self.chat_history, 'close'):
            try:
                await self.chat_history.close()
            except Exception as e:
                logger.warning(f"å…³é—­èŠå¤©å†å²ç®¡ç†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        logger.info("âœ… ExpertæœåŠ¡å·²å…³é—­")

    async def _summarize_history(self, history_messages: List[Dict]) -> str:
        """ä½¿ç”¨AIæ€»ç»“å†å²è®°å½•å†…å®¹"""
        if not history_messages:
            return ""

        # æ„å»ºå†å²è®°å½•æ–‡æœ¬
        history_text = ""
        for msg in history_messages:
            role = "ç”¨æˆ·" if msg['role'] == 'user' else "åŠ©æ‰‹"
            history_text += f"{role}: {msg['content']}\n"

        # æ„å»ºæ€»ç»“è¯·æ±‚
        summary_messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²çš„ä¸»è¦å†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œé‡ç‚¹å…³æ³¨ç”¨æˆ·çš„éœ€æ±‚å’Œå·²è®¨è®ºçš„å…³é”®ä¿¡æ¯ã€‚æ€»ç»“åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—ã€‚"
            },
            {
                "role": "user",
                "content": f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²ï¼š\n\n{history_text}"
            }
        ]

        try:
            # åˆ›å»ºAIå®¢æˆ·ç«¯è¿›è¡Œæ€»ç»“
            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.3,
                'max_tokens': 300
            }

            ai_handler = AiRequestHandler(
                messages=summary_messages,
                tools=[],  # æ€»ç»“æ—¶ä¸éœ€è¦å·¥å…·
                conversation_id=f"summary_{uuid.uuid4().hex[:8]}",
                callback=None,
                model_config=model_config
            )

            response_messages = await ai_handler.chat_completion()
            summary = response_messages[-1]['content'] if response_messages else ""

            logger.info(f"ğŸ“ å†å²è®°å½•æ€»ç»“å®Œæˆ: {summary[:50]}...")
            return summary

        except Exception as e:
            logger.error(f"âŒ å†å²è®°å½•æ€»ç»“å¤±è´¥: {e}")
            return ""

    async def ask_expert(self, question: str) -> str:
        """å‘AIä¸“å®¶æé—®"""
        try:
            logger.info(f"ğŸ¤– æ”¶åˆ°é—®é¢˜: {question[:100]}..." if len(question) > 100 else f"ğŸ¤– æ”¶åˆ°é—®é¢˜: {question}")

            # ä½¿ç”¨å›ºå®šçš„ä¼šè¯ID
            conversation_id = self.conversation_id

            # æ„å»ºsystem prompt
            system_prompt = self.system_prompt

            # å¦‚æœå¯ç”¨èŠå¤©å†å²ï¼Œè·å–å¹¶æ€»ç»“å†å²è®°å½•
            if self.enable_history:
                history_messages = await self.chat_history.get_history(conversation_id, self.history_limit)
                logger.info(f"ğŸ“ åŠ è½½å†å²è®°å½•: {len(history_messages)} æ¡")

                if history_messages:
                    # æ€»ç»“å†å²è®°å½•
                    history_summary = await self._summarize_history(history_messages)
                    if history_summary:
                        system_prompt += f"\n\nã€å¯¹è¯å†å²æ€»ç»“ã€‘\n{history_summary}"
                        logger.info(f"ğŸ“ å·²å°†å†å²è®°å½•æ€»ç»“åŠ å…¥system prompt")

                # ä¿å­˜ç”¨æˆ·é—®é¢˜
                await self.chat_history.save_message(conversation_id, "user", question)

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåªåŒ…å«system promptå’Œå½“å‰é—®é¢˜ï¼‰
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]

            logger.info(f"ğŸ¤– æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")

            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.7,
                'max_tokens': 2000
            }

            # è·å–å¯ç”¨å·¥å…·
            tools = self.mcp_tool_execute.get_tools()
            logger.info(f"ğŸ› ï¸ è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")

            logger.info(f"ğŸ¤– åˆ›å»ºAIå®¢æˆ·ç«¯ï¼Œä¼šè¯ID: {conversation_id}")

            ai_client = AiClient(
                messages, conversation_id, tools, model_config,
                None, self.mcp_tool_execute
            )

            # å¼€å§‹å¤„ç†
            logger.info(f"ğŸ¤– å¼€å§‹AIå¯¹è¯å¤„ç†")
            await ai_client.start()

            # è¿”å›æœ€åçš„åŠ©æ‰‹å›å¤
            for message in reversed(messages):
                if message.get('role') == 'assistant' and message.get('content'):
                    result = message['content']
                    # åªåœ¨å¯ç”¨å†å²è®°å½•æ—¶ä¿å­˜åŠ©æ‰‹å›å¤
                    if self.enable_history:
                        await self.chat_history.save_message(conversation_id, "assistant", result)
                        logger.info(f"ğŸ’¾ å·²ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²ï¼Œä¼šè¯ID: {conversation_id}")
                    logger.info(f"ğŸ¤– AIå›ç­”: {result[:200]}..." if len(result) > 200 else f"ğŸ¤– AIå›ç­”: {result}")
                    return result

            logger.warning(f"âš ï¸ æ²¡æœ‰è·å¾—æœ‰æ•ˆçš„AIå›å¤")
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰è·å¾—æœ‰æ•ˆå›å¤ã€‚"

        except Exception as e:
            logger.error(f"âŒ Ask expert failed: {e}")
            return f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"

    async def ask_expert_stream(self, question: str) -> AsyncGenerator[str, None]:
        """å‘AIä¸“å®¶æé—®ï¼ˆæµå¼å“åº”ï¼‰"""
        try:
            logger.info(
                f"ğŸŒŠ æ”¶åˆ°æµå¼é—®é¢˜: {question[:100]}..." if len(question) > 100 else f"ğŸŒŠ æ”¶åˆ°æµå¼é—®é¢˜: {question}")

            # ä½¿ç”¨å›ºå®šçš„ä¼šè¯ID
            conversation_id = self.conversation_id

            # æ„å»ºsystem prompt
            system_prompt = self.system_prompt

            # å¦‚æœå¯ç”¨èŠå¤©å†å²ï¼Œè·å–å¹¶æ€»ç»“å†å²è®°å½•
            if self.enable_history:
                history_messages = await self.chat_history.get_history(conversation_id, self.history_limit)
                logger.info(f"ğŸ“ æµå¼æ¨¡å¼åŠ è½½å†å²è®°å½•: {len(history_messages)} æ¡")

                if history_messages:
                    # æ€»ç»“å†å²è®°å½•
                    history_summary = await self._summarize_history(history_messages)
                    if history_summary:
                        system_prompt += f"\n\nã€å¯¹è¯å†å²æ€»ç»“ã€‘\n{history_summary}"
                        logger.info(f"ğŸ“ å·²å°†å†å²è®°å½•æ€»ç»“åŠ å…¥æµå¼system prompt")

                # ä¿å­˜ç”¨æˆ·é—®é¢˜
                await self.chat_history.save_message(conversation_id, "user", question)

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåªåŒ…å«system promptå’Œå½“å‰é—®é¢˜ï¼‰
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]

            logger.info(f"ğŸŒŠ æ„å»ºæµå¼æ¶ˆæ¯åˆ—è¡¨ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")

            model_config = {
                'api_key': self.api_key,
                'base_url': self.base_url,
                'model_name': self.model_name,
                'temperature': 0.7,
                'max_tokens': 2000
            }

            # è·å–å¯ç”¨å·¥å…·
            tools = self.mcp_tool_execute.get_tools()
            logger.info(f"ğŸ› ï¸ æµå¼æ¨¡å¼è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")

            # åˆ›å»ºæµå¼å›è°ƒ
            stream_data = []
            assistant_response = ""

            def stream_callback(event_type, data):
                logger.info(f"ğŸŒŠ æµå¼å›è°ƒ: {event_type} - {str(data)[:100]}..." if len(
                    str(data)) > 100 else f"ğŸŒŠ æµå¼å›è°ƒ: {event_type} - {data}")
                stream_data.append({"type": event_type, "data": data})

            logger.info(f"ğŸŒŠ åˆ›å»ºæµå¼AIå®¢æˆ·ç«¯ï¼Œä¼šè¯ID: {conversation_id}")

            ai_client = AiClient(
                messages, conversation_id, tools, model_config,
                stream_callback, self.mcp_tool_execute
            )

            # AIå®¢æˆ·ç«¯ç°åœ¨ç”±æ¡†æ¶ç®¡ç†

            # å¼€å§‹æµå¼å¯¹è¯
            logger.info(f"ğŸŒŠ å¼€å§‹æµå¼AIå¯¹è¯å¤„ç†")
            chunk_count = 0
            assistant_content = ""

            async for chunk in ai_client.start_stream():

                chunk_count += 1
                logger.info(f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk[:50]}..." if len(
                    chunk) > 50 else f"ğŸŒŠ äº§ç”Ÿç¬¬ {chunk_count} ä¸ªæµå¼å—: {chunk}")

                # æ”¶é›†åŠ©æ‰‹å›å¤å†…å®¹ç”¨äºä¿å­˜åˆ°å†å²è®°å½•
                try:
                    chunk_data = json.loads(chunk)
                    # ç¡®ä¿chunk_dataæ˜¯å­—å…¸ç±»å‹æ‰è°ƒç”¨.get()æ–¹æ³•
                    if isinstance(chunk_data, dict) and chunk_data.get("type") == "content" and "data" in chunk_data:
                        assistant_content += chunk_data["data"]
                except (json.JSONDecodeError, KeyError, AttributeError):
                    pass

                yield chunk

            # ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²
            if self.enable_history and assistant_content.strip():
                await self.chat_history.save_message(
                    conversation_id, "assistant", assistant_content.strip()
                )
                logger.info(f"ğŸ’¾ å·²ä¿å­˜åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²ï¼Œä¼šè¯ID: {conversation_id}")

            logger.info(f"ğŸŒŠ æµå¼å¯¹è¯å®Œæˆï¼Œæ€»å…±äº§ç”Ÿ {chunk_count} ä¸ªå—")

        except Exception as e:
            logger.error(f"âŒ Ask expert stream failed: {e}")
            yield json.dumps({"type": "error", "data": f"æŸ¥è¯¢ä¸“å®¶å¤±è´¥: {str(e)}"}, ensure_ascii=False)
